{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array, as_float_array\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxentropyMedianDichtomizationTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_splits, verbose=False):\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "        self.n_samples = None\n",
    "        self.n_features = None\n",
    "        self._splits = None\n",
    "        self._splits_indices = None\n",
    "    \n",
    "    def _check_X(self, X, n_features=None):\n",
    "        _X = None\n",
    "        if not hasattr(X, 'dtype'):\n",
    "            _X = check_array(as_float_array(X))\n",
    "        _X = check_array(X)\n",
    "        \n",
    "        if self.n_features:\n",
    "            if _X.shape[1] != self.n_features:\n",
    "                raise Exception('X has {} columns while {} are expected'.format(_X.shape[1], self.n_features))\n",
    "        return _X\n",
    "    \n",
    "    def _calc_entropy(self, X, split_bias):\n",
    "        a = np.sum(X < split_bias)\n",
    "        b = np.sum(X >= split_bias)\n",
    "\n",
    "        p = np.array([a / X.shape[0], b / X.shape[0]])\n",
    "        return -np.sum(np.log(p + 1) * p)\n",
    "\n",
    "\n",
    "    def _get_maxentropy_split(self, X):\n",
    "        # O(n^2)\n",
    "        block_size = X.shape[0]\n",
    "        ix_max_entropy = -1\n",
    "        X_diff = np.diff(X)\n",
    "\n",
    "        start_point = X.shape[0] // 2\n",
    "        \n",
    "        right_point = right_entropy = None\n",
    "        left_point = left_entropy =None\n",
    "        \n",
    "        max_entropy = -1\n",
    "        max_probas = None\n",
    "        # define point where to start looking for\n",
    "        # highest entropy\n",
    "        if X_diff[start_point] == 0:\n",
    "            _right_indices = np.where(X_diff[start_point:] > 0)[0]\n",
    "            _left_indices = np.where(X_diff[:start_point] > 0)[0]\n",
    "            \n",
    "            if _right_indices.any():\n",
    "                right_point = _right_indices[0] + start_point\n",
    "            \n",
    "            if _left_indices.any():\n",
    "                left_point = _left_indices[-1]\n",
    "            \n",
    "            # if we have constant series\n",
    "            if _right_indices is None and _left_indices is None:\n",
    "                return 0, -1\n",
    "            \n",
    "            if right_point:\n",
    "                right_entropy = self._calc_entropy(X, X[right_point])\n",
    "        \n",
    "            if left_point:\n",
    "                left_entropy = self._calc_entropy(X, X[left_point])\n",
    "        else:\n",
    "            right_point = start_point + 1\n",
    "            left_point = start_point - 1\n",
    "            \n",
    "            right_entropy = self._calc_entropy(X, X[right_point])\n",
    "            left_entropy = self._calc_entropy(X, X[left_point])\n",
    "            center_entropy = self._calc_entropy(X, X[start_point])\n",
    "            \n",
    "            if center_entropy > left_entropy and center_entropy > right_entropy:\n",
    "                return center_entropy, start_point\n",
    "\n",
    "        # if entropy at the point left to the starting point is higher\n",
    "        # search for entropy maxima\n",
    "        if right_point and  (not left_point or right_entropy > left_entropy):\n",
    "            for j in range(right_point + 1, block_size):\n",
    "                local_entropy = self._calc_entropy(X, X[j])\n",
    "                if local_entropy > right_entropy:\n",
    "                    right_point = j\n",
    "                    right_entropy = local_entropy\n",
    "                else:\n",
    "                    return right_entropy, right_point\n",
    "        elif left_point:\n",
    "            for j in reversed(range(0, left_point - 1)):\n",
    "                local_entropy = self._calc_entropy(X, X[j])\n",
    "                if local_entropy > left_entropy:\n",
    "                    left_point = j\n",
    "                    left_entropy = local_entropy\n",
    "                else:\n",
    "                    return left_entropy, left_point\n",
    "        \n",
    "        return 0, -1\n",
    "\n",
    "    def _dichtomize(self, X):\n",
    "        # O(n^2 * log n)\n",
    "        \n",
    "        _iters = np.log2(self.n_splits)\n",
    "        if _iters - int(_iters) != 0:\n",
    "            raise Exception('number of bins should be of a power of 2')\n",
    "        \n",
    "        # make first maxentropy split\n",
    "        _, initial_bin = self._get_maxentropy_split(X)\n",
    "        splits_current_feature = [(0, initial_bin), (initial_bin, self.n_samples - 1)]\n",
    "        for i in range(int(_iters) - 1):\n",
    "            # an empty list for splits in current iteration\n",
    "            _splits = list()\n",
    "            for j in splits_current_feature:\n",
    "                entropy, index = self._get_maxentropy_split(X[j[0]: j[1]])\n",
    "                if entropy == 0:\n",
    "                    _splits += [(j[0], j[1])]\n",
    "                else:\n",
    "                    _splits += [(j[0], j[0] + index), (j[0] + index, j[1])]\n",
    "\n",
    "            splits_current_feature = _splits\n",
    "            \n",
    "        return splits_current_feature\n",
    "    \n",
    "    def _convert(self, X, ix):\n",
    "        result = list()\n",
    "        for x in X.flatten():\n",
    "            result.append(np.argwhere([k[0] <= x and x < k[1] for k in self._splits[ix]]))\n",
    "        return np.array(result).reshape(-1, 1) \n",
    "    \n",
    "    def fit(self, X):\n",
    "        X = self._check_X(X)\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        self._splits = list()\n",
    "        self._splits_indices = list()\n",
    "        \n",
    "        for ix in range(self.n_features):\n",
    "            x = np.sort(X[:, ix].flatten())\n",
    "            _indices = self._dichtomize(x.flatten())\n",
    "            \n",
    "            self._splits_indices.append(_indices)\n",
    "            self._splits.append([[x[i[0]], x[i[1]]] for i in _indices])\n",
    "            \n",
    "            self._splits[-1][0][0] = -np.inf\n",
    "            self._splits[-1][-1][1] = np.inf\n",
    "            \n",
    "            self._splits = np.array(self._splits)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        _, n_features = X.shape\n",
    "        X = self._check_X(X, n_features)\n",
    "        \n",
    "        X_categorical = list()\n",
    "        for ix in range(n_features):\n",
    "            X_categorical.append(self._convert(X, ix))\n",
    "            \n",
    "        return np.hstack(X_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeFabulousTransformation(BaseEstimator):\n",
    "    def __init__(self, base_estimator, dichtomized=False, n_splits=32, exhausitve=True, random_seed=42, verbose=1):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_splits = n_splits\n",
    "        self.random_seed = random_seed\n",
    "        self.exhausitve = exhausitve\n",
    "        self.verbose = verbose\n",
    "        self.dichtomized=dichtomized\n",
    "        self._feature_space = None\n",
    "        self._feature_dichtomizers = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _cross_entropy(p, q):\n",
    "        p = np.array(p)\n",
    "        q = np.array(q)\n",
    "        q[q == 0] = 1e-9\n",
    "        return -np.sum(p * np.log(q))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _synchronize_two_dicts(_from:dict, _to:dict):\n",
    "        for i in np.setdiff1d(list(_from.keys()), list(_to.keys())):\n",
    "            if i not in _to.keys():\n",
    "                _to[i] = 0\n",
    "            \n",
    "            if i not in _from.keys():\n",
    "                _from[i] = 0\n",
    "            \n",
    "    def _dichtomize(self, X):\n",
    "        self._feature_dichtomizers = list()\n",
    "        self._feature_space = list()\n",
    "        \n",
    "        if not self.dichtomized:\n",
    "            for i in range(X.shape[1]):\n",
    "                feature = X[:, i].reshape(-1, 1)\n",
    "                dichtomizer = MaxentropyMedianDichtomizationTransformer(32).fit(feature)\n",
    "                feature_dichtomized = dichtomizer.transform(feature)\n",
    "                onehot_encoder = OneHotEncoder(sparse=True).fit(feature_dichtomized)\n",
    "\n",
    "                self._feature_dichtomizers.append({'dichtomizer': dichtomizer, 'encoder': onehot_encoder})\n",
    "                self._feature_space.append({'categorical': feature_dichtomized, 'binary': onehot_encoder.transform(feature_dichtomized)})\n",
    "        else:\n",
    "            for i in range(X.shape[1]):\n",
    "                feature = X[:, i].reshape(-1, 1)\n",
    "                onehot_encoder = OneHotEncoder(sparse=True).fit(feature)\n",
    "                self._feature_dichtomizers.append({'dichtomizer': None, 'encoder': onehot_encoder})\n",
    "                self._feature_space.append({'categorical': feature, 'binary': onehot_encoder.transform(feature)})\n",
    "                \n",
    "    \n",
    "    def _calc_cross_entropy(self, ix_feature, pred, dataset_size):\n",
    "        real = np.asarray(self._feature_space[ix_feature]['binary'].sum(1))\n",
    "        pred = np.asarray(pred.sum(1))\n",
    "        \n",
    "        pred[np.argwhere(pred == 2)] = 1\n",
    "        \n",
    "        pred_category, pred_counts = np.unique(pred, return_counts=True)\n",
    "        real_category, real_counts = np.unique(real, return_counts=True)\n",
    "\n",
    "        pred_proba = pred_counts / dataset_size\n",
    "        real_proba = real_counts / dataset_size\n",
    "\n",
    "        real_stats = dict(zip(real_category, real_proba))\n",
    "        pred_stats =  dict(zip(pred_category, pred_proba))\n",
    "\n",
    "        SomeFabulousTransformation._synchronize_two_dicts(real_stats, pred_stats)\n",
    "        \n",
    "        return SomeFabulousTransformation._cross_entropy(list(real_stats.values()), list(pred_stats.values()))\n",
    "    \n",
    "    def _fit_transform(self, X, initial_feature_ix):\n",
    "        dataset_size = X.shape[0]\n",
    "        free_features_ix = [i for i in range(len(self._feature_space)) if i != initial_feature_ix]\n",
    "        active_features_subset = [self._feature_space[initial_feature_ix]['binary']]\n",
    "        active_features_subset_ix = [initial_feature_ix]\n",
    "        \n",
    "        while len(active_features_subset) != len(self._feature_space):\n",
    "            max_entropy = -1\n",
    "            max_entropy_feature_ix = -1\n",
    "            max_entropy_feature_value = None\n",
    "            \n",
    "            if self.verbose > 1:\n",
    "                print('currently processed {} features out of {}'.format(len(active_features_subset), len(self._feature_space)))\n",
    "                print('number of active features {}'.format(len(active_features_subset)))\n",
    "            \n",
    "            for ix_current_feature in free_features_ix:\n",
    "                if len(active_features_subset) > 1:\n",
    "                    model_input_features = sparse.hstack(active_features_subset)\n",
    "                else:\n",
    "                    model_input_features = active_features_subset[0]\n",
    "                \n",
    "                estimator = copy(self.base_estimator)\n",
    "                estimator.fit(model_input_features, self._feature_space[ix_current_feature]['categorical'].squeeze())\n",
    "                \n",
    "                pred = estimator.predict(model_input_features)\n",
    "\n",
    "                pred_onehot = self._feature_dichtomizers[ix_current_feature]['encoder'].transform(pred.reshape(-1, 1))\n",
    "                pred_diff = (pred_onehot != self._feature_space[ix_current_feature]['binary']).astype(np.int32)\n",
    "                \n",
    "                entropy = self._calc_cross_entropy(ix_current_feature, pred_diff, dataset_size)\n",
    "                \n",
    "                if entropy > max_entropy:\n",
    "                    max_entropy_feature_value = pred_diff\n",
    "                    max_entropy_feature_ix = ix_current_feature\n",
    "                    max_entropy = entropy\n",
    "                    \n",
    "            free_features_ix.remove(max_entropy_feature_ix)\n",
    "            active_features_subset.append(max_entropy_feature_value)\n",
    "            active_features_subset_ix.append(max_entropy_feature_ix)\n",
    "            \n",
    "        return np.hstack(active_features_subset), active_features_subset_ix      \n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        self._dichtomize(X)\n",
    "        \n",
    "        if not self.exhausitve:\n",
    "            initial_feature_ix = np.random.randint(0, len(self._feature_space))\n",
    "            return self._fit_transform(X, initial_feature_ix)\n",
    "        \n",
    "        features_subset = list()\n",
    "        features_subset_ix = list()\n",
    "        for initial_feature_ix in range(len(self._feature_space)):\n",
    "            if self.verbose == 1:\n",
    "                print('processing starting feature {}'.format(initial_feature_ix))\n",
    "            _features_subset, _features_subset_ix = self._fit_transform(X, initial_feature_ix)\n",
    "            \n",
    "            features_subset.append(_features_subset)\n",
    "            features_subset_ix.append(_features_subset_ix)\n",
    "            \n",
    "        return features_subset, np.vstack(features_subset_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперимент на синтетике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coefs = make_regression(n_samples=1000, n_features=8, n_informative=4, n_targets=3, coef=True, random_state=42)\n",
    "X = np.concatenate([X, y[:, :-1]], axis=1)\n",
    "y = y[:, -1]#.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'rf': LGBMClassifier(boosting_type='rf', bagging_freq=5, bagging_fraction=.05, feature_fraction=.1),\n",
    "    'lr': LogisticRegression(random_state=42, multi_class='ovr', solver='liblinear', C=10000, tol=1e-2, n_jobs=-1),\n",
    "    'svc': LinearSVC(multi_class='ovr'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = SomeFabulousTransformation(models['lr'], exhausitve=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing starting feature 0\n",
      "processing starting feature 1\n",
      "processing starting feature 2\n",
      "processing starting feature 3\n",
      "processing starting feature 4\n",
      "processing starting feature 5\n",
      "processing starting feature 6\n",
      "processing starting feature 7\n",
      "processing starting feature 8\n",
      "processing starting feature 9\n"
     ]
    }
   ],
   "source": [
    "features, indices = transform.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 6, 8, 5, 1, 7, 9, 4, 2],\n",
       "       [1, 9, 6, 3, 8, 5, 0, 2, 4, 7],\n",
       "       [2, 4, 3, 0, 8, 9, 6, 1, 7, 5],\n",
       "       [3, 0, 4, 1, 8, 5, 7, 9, 2, 6],\n",
       "       [4, 1, 5, 6, 9, 2, 8, 7, 3, 0],\n",
       "       [5, 1, 7, 4, 9, 2, 8, 0, 6, 3],\n",
       "       [6, 9, 1, 3, 4, 5, 8, 2, 0, 7],\n",
       "       [7, 8, 6, 2, 0, 4, 9, 1, 5, 3],\n",
       "       [8, 7, 6, 9, 0, 5, 2, 1, 4, 3],\n",
       "       [9, 6, 1, 0, 8, 5, 7, 3, 4, 2]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 5, 8, 9]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(coefs.sum(1) > 0)[0].tolist() + [8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ix = np.unique(indices).tolist()\n",
    "ix_counts = {i: 0 for i in unique_ix}\n",
    "\n",
    "for row in indices:\n",
    "    ranks = dict(zip(row.tolist(), reversed(range(row.shape[0]))))\n",
    "    for i, j in ranks.items():\n",
    "        ix_counts[i] += j / (row.shape[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x153caf7f208>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(ix_counts).sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
