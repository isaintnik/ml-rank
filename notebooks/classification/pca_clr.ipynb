{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF regressor optimization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt for lightgbm shows terrible results\n",
    "def hyperopt_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    space = hp.choice('clr_type', [\n",
    "        {\n",
    "            'type': 'lightgbm',\n",
    "            'feature_fraction': hp.uniform('feature_fraction', 0.05, 0.95),\n",
    "            'bagging_fraction': hp.uniform('bagging_fraction', 0.05, 0.95),\n",
    "            'bagging_freq': hp.uniform('bagging_freq', 1, 50),\n",
    "            'n_estimators': hp.uniform('n_estimators', 5, 50),\n",
    "            #'max_bin': hp.uniform('max_bin', )\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    best = fmin(\n",
    "        fn=lambda args: cross_val_score(\n",
    "            LGBMClassifier(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=args['feature_fraction'], \n",
    "                bagging_freq=int(args['bagging_freq']), \n",
    "                bagging_fraction=args['bagging_fraction'],\n",
    "                n_estimators=int(args['n_estimators'])\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='accuracy'\n",
    "        ).mean(),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_iter_opt\n",
    "    )\n",
    "    \n",
    "    return best\n",
    "\n",
    "def bayesian_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    svr_opt = BayesianOptimization(\n",
    "        lambda feature_fraction, bagging_freq, bagging_fraction, n_estimators: cross_val_score(\n",
    "            LGBMClassifier(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=feature_fraction, \n",
    "                bagging_freq=int(bagging_freq), \n",
    "                bagging_fraction=bagging_fraction,\n",
    "                n_estimators=int(n_estimators)\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='accuracy'\n",
    "        ).mean(),\n",
    "        {'feature_fraction': (0.05, 0.95),\n",
    "         'bagging_fraction': (0.05, 0.95),\n",
    "         'bagging_freq': (1, 50),\n",
    "         'n_estimators': (5, 50) },\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    svr_opt.init(10)\n",
    "    svr_opt.maximize(n_iter=max_iter_opt)\n",
    "    \n",
    "    return svr_opt.res['max']['max_params']#['C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = read_idx('./datasets/mnist/train-images.idx3-ubyte')\n",
    "y = read_idx('./datasets/mnist/train-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / np.max(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training plain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_train, y_train, cv=4, max_iter_opt=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'rf': LGBMClassifier(boosting_type='rf', **params_opt),\n",
    "    'lr': LogisticRegression(random_state=42, multi_class='ovr', solver='liblinear', C=10000, tol=1e-2),\n",
    "    'svc': LinearSVC(multi_class='ovr', C=10000, tol=1e-2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf is ready\n",
      "lr is ready\n",
      "svc is ready\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=6, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_train, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_train[train_ix], X_train[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))\n",
    "    print(k + ' is ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (averate scores):\n",
      "rf, Accuracy: 0.9244629629629629\n",
      "lr, Accuracy: 0.9156296296296297\n",
      "svc, Accuracy: 0.8632962962962963\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (validation scores):\n",
      "rf, Accuracy: 0.9288333333333333\n",
      "lr, Accuracy: 0.9158333333333334\n",
      "svc, Accuracy: 0.8576666666666667\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with PCA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 10\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca, y_train = shuffle(X_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_pca, y_train, cv=4, max_iter_opt=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'svr': LinearSVC(C=10000, dual=False, max_iter=100, tol=1e-2),\n",
    "    'linear': LogisticRegression(random_state=42, multi_class='ovr', solver='sag', C=10000, tol=1e-2, n_jobs=10),\n",
    "    'forest': LGBMClassifier(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr is ready\n",
      "linear is ready\n",
      "forest is ready\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=6, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_pca, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_pca[train_ix], X_pca[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))\n",
    "    print(k + ' is ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for PCA data (averate scores):\n",
      "svr, Accuracy: 0.7684074074074073\n",
      "linear, Accuracy: 0.7775555555555554\n",
      "forest, Accuracy: 0.8348333333333334\n"
     ]
    }
   ],
   "source": [
    "print('for PCA data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using PCA (validation scores):\n",
      "svr, Accuracy: 0.7608333333333334\n",
      "linear, Accuracy: 0.7696666666666667\n",
      "forest, Accuracy: 0.8371666666666666\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using PCA (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_pca, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val_pca), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/creditcard/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[np.setdiff1d(df.columns, [\"Class\", \"Time\"])].values\n",
    "y = df.Class.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (284807, 29)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training plain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_train, y_train, cv=4, max_iter_opt=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'svr': LinearSVC(C=10000, dual=False, max_iter=100, tol=1e-2),\n",
    "    'linear': LogisticRegression(random_state=42, multi_class='ovr', solver='sag', C=10000, tol=1e-2, n_jobs=10),\n",
    "    'forest': LGBMClassifier(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr is ready\n",
      "linear is ready\n",
      "forest is ready\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=6, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_train, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_train[train_ix], X_train[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))\n",
    "    print(k + ' is ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (averate scores):\n",
      "svr, Accuracy: 0.9991768295061757\n",
      "linear, Accuracy: 0.9981859038880176\n",
      "forest, Accuracy: 0.9994772282171921\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (validation scores):\n",
      "svr, Accuracy: 0.9992977774656788\n",
      "linear, Accuracy: 0.9981742214107651\n",
      "forest, Accuracy: 0.9996488887328394\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with PCA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 10\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca, y_train = shuffle(X_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_pca, y_train, cv=4, max_iter_opt=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LogisticRegression(),\n",
    "    'svr': LinearSVC(C=100, dual=False, max_iter=200),\n",
    "    'forest': LGBMClassifier(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=6, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_pca, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_pca[train_ix], X_pca[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for PCA data (averate scores):\n",
      "linear, Accuracy: 0.9989817654081131\n",
      "svr, Accuracy: 0.9989232461786943\n",
      "forest, Accuracy: 0.9992665589912845\n"
     ]
    }
   ],
   "source": [
    "print('for PCA data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using PCA (validation scores):\n",
      "linear, Accuracy: 0.9990519995786665\n",
      "svr, Accuracy: 0.9988413328183702\n",
      "forest, Accuracy: 0.9993328885923949\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using PCA (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_pca, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val_pca), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/cancer/breast_cancer.csv')\n",
    "y = df.diagnosis.replace('M', 0).replace('B', 1).values\n",
    "X = np.asarray(df.drop(['diagnosis', 'id', 'Unnamed: 32'], axis=1).as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training plain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X, y, cv=4, max_iter_opt=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'rf': LGBMClassifier(boosting_type='rf', **params_opt),\n",
    "    'lr': LogisticRegression(random_state=42, multi_class='ovr', solver='liblinear', C=10000, tol=1e-2),\n",
    "    'svc': LinearSVC(multi_class='ovr', C=10000, tol=1e-2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf is ready\n",
      "lr is ready\n",
      "svc is ready\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=6, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_train, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_train[train_ix], X_train[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))\n",
    "    print(k + ' is ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (averate scores):\n",
      "rf, Accuracy: 0.955061559507524\n",
      "lr, Accuracy: 0.9414728682170543\n",
      "svc, Accuracy: 0.8336297309621523\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (validation scores):\n",
      "rf, Accuracy: 0.8947368421052632\n",
      "lr, Accuracy: 0.8771929824561403\n",
      "svc, Accuracy: 0.8245614035087719\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with PCA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 10\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca, y_train = shuffle(X_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_pca, y_train, cv=4, max_iter_opt=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'rf': LGBMClassifier(boosting_type='rf', **params_opt),\n",
    "    'lr': LogisticRegression(random_state=42, multi_class='ovr', solver='liblinear', C=10000, tol=1e-2),\n",
    "    'svc': LinearSVC(multi_class='ovr', C=10000, tol=1e-2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=6, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_pca, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_pca[train_ix], X_pca[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for PCA data (averate scores):\n",
      "rf, Accuracy: 0.9453944368445053\n",
      "lr, Accuracy: 0.9355905152758779\n",
      "svc, Accuracy: 0.9102143182854538\n"
     ]
    }
   ],
   "source": [
    "print('for PCA data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using PCA (validation scores):\n",
      "rf, Accuracy: 0.9122807017543859\n",
      "lr, Accuracy: 0.8421052631578947\n",
      "svc, Accuracy: 0.8596491228070176\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using PCA (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_pca, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val_pca), y_val.squeeze())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
