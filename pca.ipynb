{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "# https://lightgbm.readthedocs.io/en/latest/Python-API.html#lightgbm.LGBMModel\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR optimization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimization_svr(X, y, cv=6, max_iter_svr=100, max_iter_opt=15):\n",
    "    svr_opt = BayesianOptimization(\n",
    "        lambda epsilon: cross_val_score(\n",
    "            LinearSVR(C=100, epsilon=epsilon, max_iter=max_iter_svr),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='neg_mean_squared_error'\n",
    "        ).mean(),\n",
    "        {'epsilon': (0.0, 5.0)},\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    svr_opt.init(10)\n",
    "    svr_opt.maximize(n_iter=max_iter_opt)\n",
    "    \n",
    "    return svr_opt.res['max']['max_params']['epsilon']\n",
    "\n",
    "def hyperopt_optimization_svr(X, y, cv=6, max_iter_svr=100, max_iter_opt=15):\n",
    "    space = hp.choice('regressor_type', [\n",
    "        {\n",
    "            'type': 'svr',\n",
    "            'epsilon': hp.uniform('svr_epsilon', 0.0, 5.0)\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    best = fmin(\n",
    "        fn=lambda args: cross_val_score(\n",
    "            LinearSVR(C=100, epsilon=args['epsilon'], max_iter=max_iter_svr),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='neg_mean_squared_error'\n",
    "        ).mean(),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_iter_opt\n",
    "    )\n",
    "\n",
    "    return best['svr_epsilon']\n",
    "\n",
    "def evaulate_best_svr_argument(X, y, eps_vals: list, cv=6):\n",
    "    scores = []\n",
    "    for e in eps_vals:\n",
    "        scores.append(cross_val_score(\n",
    "            LinearSVR(C=100, epsilon=e, max_iter=200),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='neg_mean_squared_error'\n",
    "        ).mean())\n",
    "        \n",
    "    return eps_vals[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF regressor optimization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt for lightgbm shows terrible results\n",
    "def hyperopt_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    space = hp.choice('regressor_type', [\n",
    "        {\n",
    "            'type': 'lightgbm',\n",
    "            'feature_fraction': hp.uniform('feature_fraction', 0.05, 0.95),\n",
    "            'bagging_fraction': hp.uniform('bagging_fraction', 0.05, 0.95),\n",
    "            'bagging_freq': hp.uniform('bagging_freq', 1, 50),\n",
    "            'n_estimators': hp.uniform('n_estimators', 5, 50),\n",
    "            #'max_bin': hp.uniform('max_bin', )\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    best = fmin(\n",
    "        fn=lambda args: cross_val_score(\n",
    "            LGBMRegressor(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=args['feature_fraction'], \n",
    "                bagging_freq=int(args['bagging_freq']), \n",
    "                bagging_fraction=args['bagging_fraction'],\n",
    "                n_estimators=int(args['n_estimators'])\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='neg_mean_squared_error'\n",
    "        ).mean(),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_iter_opt\n",
    "    )\n",
    "    \n",
    "    return best\n",
    "\n",
    "def bayesian_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    svr_opt = BayesianOptimization(\n",
    "        lambda feature_fraction, bagging_freq, bagging_fraction, n_estimators: cross_val_score(\n",
    "            LGBMRegressor(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=feature_fraction, \n",
    "                bagging_freq=int(bagging_freq), \n",
    "                bagging_fraction=bagging_fraction,\n",
    "                n_estimators=int(n_estimators)\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='neg_mean_squared_error'\n",
    "        ).mean(),\n",
    "        {'feature_fraction': (0.05, 0.95),\n",
    "         'bagging_fraction': (0.05, 0.95),\n",
    "         'bagging_freq': (1, 50),\n",
    "         'n_estimators': (5, 50) },\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    svr_opt.init(10)\n",
    "    svr_opt.maximize(n_iter=max_iter_opt)\n",
    "    \n",
    "    return svr_opt.res['max']['max_params']#['C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Comment Volume Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/facebook_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (40949, 54)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical variables\n",
    "# H Local - category\n",
    "# Post Promotion Status - category\n",
    "# Base Time - time variable\n",
    "# Page Category - category\n",
    "\n",
    "H_Local = OneHotEncoder().fit_transform(df['H Local'].values.reshape(-1, 1)).todense()\n",
    "Post_Promotion_Status = OneHotEncoder().fit_transform(df['Post Promotion Status'].values.reshape(-1, 1)).todense()\n",
    "Base_Time = OneHotEncoder().fit_transform(df['Base Time'].values.reshape(-1, 1)).todense()\n",
    "Page_Category = OneHotEncoder().fit_transform(df['Page Category'].values.reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Target.values.reshape(-1, 1)\n",
    "X = df.drop(['H Local', 'Post Promotion Status', 'Base Time', 'Page Category', 'Target'], axis=1).values\n",
    "X = np.hstack([X, H_Local, Post_Promotion_Status, Base_Time, Page_Category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after preprocessing: (40949, 228)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size after preprocessing: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training plain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating SVR penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_hyperopt = hyperopt_optimization_svr(X_train, y_train, cv=4, max_iter_svr=200, max_iter_opt=15)\n",
    "eps_bayesian = bayesian_optimization_svr(X_train, y_train, cv=4, max_iter_svr=200, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_opt = evaulate_best_svr_argument(X_train, y_train, [eps_hyperopt, eps_bayesian], cv=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-3.39073783]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 51, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    }
   ],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_train, y_train, cv=6, max_iter_opt=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LinearRegression(),\n",
    "    'svr': LinearSVR(C=100, epsilon=eps_opt, max_iter=200),\n",
    "    'forest': LGBMRegressor(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_train, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_train[train_ix], X_train[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(mean_squared_error(model.predict(X_crossval_test), y_crossval_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (averate scores):\n",
      "linear, MSE: 795.2162610416704\n",
      "svr, MSE: 1728.8641361699947\n",
      "forest, MSE: 772.6272131333926\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, MSE: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (validation scores):\n",
      "linear, MSE: 914.2573871292184\n",
      "svr, MSE: 275536.7747979826\n",
      "forest, MSE: 977.3517320750891\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train.squeeze())\n",
    "    print('{}, MSE: {}'.format(name, mean_squared_error(model.predict(X_val), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with PCA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 10\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca, y_train = shuffle(X_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating SVR penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_hyperopt = hyperopt_optimization_svr(X_pca, y_train, cv=4, max_iter_svr=200, max_iter_opt=15)\n",
    "C_bayesian = bayesian_optimization_svr(X_pca, y_train, cv=4, max_iter_svr=200, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_opt = evaulate_best_svr_argument(X_pca, y_train, [C_hyperopt, C_bayesian], cv=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([0.90489854]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 49, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.70217373]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 50, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    }
   ],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_pca, y_train, cv=6, max_iter_opt=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LinearRegression(),\n",
    "    'svr': LinearSVR(C=C_opt),\n",
    "    'forest': LGBMRegressor(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_pca, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_pca[train_ix], X_pca[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(mean_squared_error(model.predict(X_crossval_test), y_crossval_test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using PCA (averate scores)\n",
      "linear, MSE: 907.6697363284552\n",
      "svr, MSE: 185357.7382693561\n",
      "forest, MSE: 910.0725575075838\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using PCA (averate scores)')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, MSE: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using PCA (validation scores):\n",
      "linear, MSE: 1464.9637832200358\n",
      "svr, MSE: 95049.85683415309\n",
      "forest, MSE: 1473.1972658494396\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using PCA (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train.squeeze())\n",
    "    print('{}, MSE: {}'.format(name, mean_squared_error(model.predict(X_val), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parkinsons Telemonitoring Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/parkinsons_updrs.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (5875, 22)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = df['subject#'].unique().tolist()\n",
    "subject_binary = np.zeros((df.shape[0], len(subject)))\n",
    "for k, i in df.iterrows():\n",
    "    subject_binary[k, int(i['subject#']) - 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing categorical features with binary values\n",
    "y = df.total_UPDRS.values\n",
    "X = df.drop(['motor_UPDRS', 'total_UPDRS', 'subject#'], axis=1).values\n",
    "X = np.concatenate([subject_binary, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after preprocessing: (5875, 61)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size after preprocessing: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training plain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating SVR penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_hyperopt = hyperopt_optimization_svr(X_train, y_train, cv=6, max_iter_svr=1000, max_iter_opt=15)\n",
    "C_bayesian = bayesian_optimization_svr(X_train, y_train, cv=6, max_iter_svr=1000, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_opt = evaulate_best_svr_argument(X_train, y_train, [C_hyperopt, C_bayesian], cv=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_train, y_train, cv=6, max_iter_opt=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LinearRegression(),\n",
    "    'svr': LinearSVR(C=C_opt, max_iter=1000),\n",
    "    'forest': LGBMRegressor(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_train, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_train[train_ix], X_train[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(mean_squared_error(model.predict(X_crossval_test), y_crossval_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (averate scores):\n",
      "linear, MSE: 6.813584776542285\n",
      "svr, MSE: 17.60012254051052\n",
      "forest, MSE: 8.009305240274019\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, MSE: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (validation scores):\n",
      "linear, MSE: 6.098093058357888\n",
      "svr, MSE: 9.046091209340968\n",
      "forest, MSE: 7.484697650054532\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train.squeeze())\n",
    "    print('{}, MSE: {}'.format(name, mean_squared_error(model.predict(X_val), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with PCA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 10\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca, y_train = shuffle(X_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating SVR penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_hyperopt = hyperopt_optimization_svr(X_pca, y_train, cv=6, max_iter_svr=1000, max_iter_opt=15)\n",
    "C_bayesian = bayesian_optimization_svr(X_pca, y_train, cv=6, max_iter_svr=1000, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_opt = evaulate_best_svr_argument(X_pca, y_train, [C_hyperopt, C_bayesian], cv=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_pca, y_train, cv=6, max_iter_opt=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LinearRegression(),\n",
    "    'svr': LinearSVR(C=C_opt),\n",
    "    'forest': LGBMRegressor(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_pca, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_pca[train_ix], X_pca[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(mean_squared_error(model.predict(X_crossval_test), y_crossval_test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using PCA (averate scores)\n",
      "linear, MSE: 83.11222503906008\n",
      "svr, MSE: 94.73598804808401\n",
      "forest, MSE: 8.420770416791695\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using PCA (averate scores)')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, MSE: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using PCA (validation scores):\n",
      "linear, MSE: 121.4606121530821\n",
      "svr, MSE: 125.52757627239173\n",
      "forest, MSE: 120.1179463432551\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using PCA (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train.squeeze())\n",
    "    print('{}, MSE: {}'.format(name, mean_squared_error(model.predict(X_val), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy efficiency Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./datasets/ENB2012_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (768, 10)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = df['X6'].unique().tolist()\n",
    "subject_map = dict(zip(subject, range(len(subject))))\n",
    "subject_binary = np.zeros((df.shape[0], len(subject)))\n",
    "for k, i in df.iterrows():\n",
    "    subject_binary[k, subject_map[i['X6']]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing categorical features with binary values\n",
    "y = df.Y1.values\n",
    "X = df.drop(['Y1', 'Y2', 'X6'], axis=1).values\n",
    "X = np.concatenate([subject_binary, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after preprocessing: (768, 11)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size after preprocessing: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training plain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating SVR penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_hyperopt = hyperopt_optimization_svr(X_train, y_train, cv=6, max_iter_svr=1000, max_iter_opt=15)\n",
    "C_bayesian = bayesian_optimization_svr(X_train, y_train, cv=6, max_iter_svr=1000, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_opt = evaulate_best_svr_argument(X_train, y_train, [C_hyperopt, C_bayesian], cv=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-16.16280141]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 29, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    }
   ],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_train, y_train, cv=6, max_iter_opt=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LinearRegression(),\n",
    "    'svr': LinearSVR(C=C_opt, max_iter=1000),\n",
    "    'forest': LGBMRegressor(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_train, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_train[train_ix], X_train[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(mean_squared_error(model.predict(X_crossval_test), y_crossval_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (averate scores):\n",
      "linear, MSE: 8.986799494934631\n",
      "svr, MSE: 32.901727228720524\n",
      "forest, MSE: 1.0745437484051013\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, MSE: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (validation scores):\n",
      "linear, MSE: 7.748646909011892\n",
      "svr, MSE: 17.127049376679032\n",
      "forest, MSE: 1.3444327311825048\n"
     ]
    }
   ],
   "source": [
    "print('for pure data (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train.squeeze())\n",
    "    print('{}, MSE: {}'.format(name, mean_squared_error(model.predict(X_val), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with PCA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 4\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating SVR penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca, y_train = shuffle(X_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-1.583638e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 50, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    }
   ],
   "source": [
    "C_hyperopt = hyperopt_optimization_svr(X_pca, y_train, cv=6, max_iter_svr=1000, max_iter_opt=15)\n",
    "C_bayesian = bayesian_optimization_svr(X_pca, y_train, cv=6, max_iter_svr=1000, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_opt = evaulate_best_svr_argument(X_pca, y_train, [C_hyperopt, C_bayesian], cv=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_pca, y_train, cv=6, max_iter_opt=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LinearRegression(),\n",
    "    'svr': LinearSVR(C=C_opt),\n",
    "    'forest': LGBMRegressor(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_pca, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_pca[train_ix], X_pca[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(mean_squared_error(model.predict(X_crossval_test), y_crossval_test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using PCA (averate scores)\n",
      "linear, MSE: 21.11891304611659\n",
      "svr, MSE: 59.04677932262165\n",
      "forest, MSE: 6.704298941195557\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using PCA (averate scores)')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, MSE: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using PCA (validation scores):\n",
      "linear, MSE: 103.87685186956993\n",
      "svr, MSE: 152.16182537274634\n",
      "forest, MSE: 107.50667933196301\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using PCA (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train.squeeze())\n",
    "    print('{}, MSE: {}'.format(name, mean_squared_error(model.predict(X_val), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
