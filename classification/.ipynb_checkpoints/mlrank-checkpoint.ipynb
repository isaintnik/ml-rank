{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "# https://lightgbm.readthedocs.io/en/latest/Python-API.html#lightgbm.LGBMModel\n",
    "from lightgbm import LGBMClassifier \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from copy import deepcopy as copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс дискретизации по принципу деления по медиане"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedianBinarizer(object):\n",
    "    def __init__(self, n_bins: int):\n",
    "        self.n_bins = n_bins\n",
    "        self.x_bins = None\n",
    "        self.encoder = None\n",
    "    \n",
    "    def _get_feature_bins(self, X):\n",
    "        #extra_bin = 0\n",
    "        #if X.shape[0] % self.n_bins != 0:\n",
    "        #    extra_bin = 1\n",
    "        #    print('number of bins does not fit to dataset'\n",
    "        #          ', extending number of bins to extra one')\n",
    "        #\n",
    "        #n_batch = X.shape[0] // self.n_bins\n",
    "        #X_sorted = np.sort(X)\n",
    "        #x_bins = [X[i*n_batch:(i+1)*n_batch] for i in range(self.n_bins + extra_bin)]\n",
    "        \n",
    "        iterations = np.log2(self.n_bins)\n",
    "        if iterations - int(iterations) != 0:\n",
    "            raise Exception('bins should be a power of 2')\n",
    "        \n",
    "        # получаем вариационный ряд\n",
    "        x_bins = [np.sort(X)]\n",
    "        for i in range(int(iterations)):\n",
    "            new_bins = list()\n",
    "            for j in x_bins:\n",
    "                # середина среза вариационного ряда\n",
    "                index = j.shape[0] // 2\n",
    "                new_bins += [j[:index], j[index:]]\n",
    "            x_bins = new_bins\n",
    "        return x_bins\n",
    "    \n",
    "    def _lookup(self, x):\n",
    "        for k, _bin in enumerate(self.x_bins):\n",
    "            if x <= _bin[-1]:\n",
    "                return k\n",
    "        return k\n",
    "    \n",
    "    def _real_to_category(self, X):\n",
    "        # X should not be a column vector\n",
    "        X_new = []\n",
    "        for x in X:\n",
    "            i = self._lookup(x)\n",
    "            \n",
    "            if i != -1:\n",
    "                X_new.append(i)\n",
    "            else:\n",
    "                raise Exception('something went wrong')\n",
    "\n",
    "        return np.array(X_new).reshape(-1, 1)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.x_bins = self._get_feature_bins(X)\n",
    "        X_cat = self._real_to_category(X)\n",
    "\n",
    "        self.encoder = OneHotEncoder(n_values=self.n_bins, sparse=False)\n",
    "        self.encoder.fit(X_cat)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform_categorical(self, X):\n",
    "        return self._real_to_category(X)\n",
    "    \n",
    "    def transform_onehot(self, X):\n",
    "        x_cat = self._real_to_category(X)\n",
    "        return self.encoder.transform(x_cat)\n",
    "    \n",
    "    def onehot_to_categorical(self, X):\n",
    "        return np.argwhere(X)[:, 1]\n",
    "    \n",
    "    def categorical_to_onehot(self, X):\n",
    "        return self.encoder.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF regressor optimization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt for lightgbm shows terrible results\n",
    "def hyperopt_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    space = hp.choice('clr_type', [\n",
    "        {\n",
    "            'type': 'lightgbm',\n",
    "            'feature_fraction': hp.uniform('feature_fraction', 0.05, 0.95),\n",
    "            'bagging_fraction': hp.uniform('bagging_fraction', 0.05, 0.95),\n",
    "            'bagging_freq': hp.uniform('bagging_freq', 1, 50),\n",
    "            'n_estimators': hp.uniform('n_estimators', 5, 50),\n",
    "            #'max_bin': hp.uniform('max_bin', )\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    best = fmin(\n",
    "        fn=lambda args: cross_val_score(\n",
    "            LGBMClassifier(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=args['feature_fraction'], \n",
    "                bagging_freq=int(args['bagging_freq']), \n",
    "                bagging_fraction=args['bagging_fraction'],\n",
    "                n_estimators=int(args['n_estimators'])\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='accuracy'\n",
    "        ).mean(),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_iter_opt\n",
    "    )\n",
    "    \n",
    "    return best\n",
    "\n",
    "def bayesian_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    svr_opt = BayesianOptimization(\n",
    "        lambda feature_fraction, bagging_freq, bagging_fraction, n_estimators: cross_val_score(\n",
    "            LGBMClassifier(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=feature_fraction, \n",
    "                bagging_freq=int(bagging_freq), \n",
    "                bagging_fraction=bagging_fraction,\n",
    "                n_estimators=int(n_estimators)\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='accuracy'\n",
    "        ).mean(),\n",
    "        {'feature_fraction': (0.05, 0.95),\n",
    "         'bagging_fraction': (0.05, 0.95),\n",
    "         'bagging_freq': (1, 50),\n",
    "         'n_estimators': (5, 50) },\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    svr_opt.init(10)\n",
    "    svr_opt.maximize(n_iter=max_iter_opt)\n",
    "    \n",
    "    return svr_opt.res['max']['max_params']#['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronize_counts(_from: dict, _to: dict):\n",
    "    subset = np.setdiff1d(list(_from.keys()), list(_to.keys()))\n",
    "    for i in subset:\n",
    "        _to[i] = 0\n",
    "        \n",
    "def cross_entropy(p, q):\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    q[q == 0] = 0.0001\n",
    "    return -np.sum(p * np.log(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Comment Volume Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/facebook_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (40949, 54)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature Post Promotion Status is ignored\n",
      "feature base_weekday_1 is ignored\n",
      "feature published_weekday_0 is ignored\n"
     ]
    }
   ],
   "source": [
    "y = MedianBinarizer(n_bins=32).fit(df.Target.values.reshape(-1, 1)).transform_categorical(df.Target.values.reshape(-1, 1))\n",
    "df.drop(['Target'], axis=1, inplace=True)\n",
    "\n",
    "categorical_features = ['H Local', 'Post Promotion Status', 'Base Time', 'Page Category']\n",
    "\n",
    "\n",
    "features_categorical = list()\n",
    "features_real = list()\n",
    "encoders = list()\n",
    "\n",
    "for i in categorical_features:\n",
    "    encoder = OneHotEncoder().fit(df[i].values.reshape(-1, 1))\n",
    "    \n",
    "    onehot_repr = encoder.transform(df[i].values.reshape(-1, 1)).todense()\n",
    "    categorical_repr = df[i].values.reshape(-1, 1)\n",
    "    \n",
    "    if np.unique(categorical_repr).size < 2:\n",
    "        print('feature {} is ignored'.format(i))\n",
    "        continue\n",
    "        \n",
    "    encoders.append(encoder)\n",
    "    \n",
    "    features_categorical.append({\n",
    "        'onehot': onehot_repr,\n",
    "        'category': categorical_repr\n",
    "    })\n",
    "\n",
    "for i in np.setdiff1d(ar1=df.columns, ar2=categorical_features):\n",
    "    binarizer = MedianBinarizer(n_bins=32).fit(df[i].values.reshape(-1, 1))\n",
    "    \n",
    "    onehot_repr = binarizer.transform_onehot(df[i].values.reshape(-1, 1)),\n",
    "    categorical_repr = binarizer.transform_categorical(df[i].values.reshape(-1, 1))\n",
    "    \n",
    "    if np.unique(categorical_repr).size < 2:\n",
    "        print('feature {} is ignored'.format(i))\n",
    "        continue\n",
    "        \n",
    "    encoders.append(binarizer.encoder)\n",
    "    \n",
    "    features_real.append({\n",
    "        'onehot': binarizer.transform_onehot(df[i].values.reshape(-1, 1)),\n",
    "        'category': binarizer.transform_categorical(df[i].values.reshape(-1, 1))\n",
    "    })\n",
    "\n",
    "feature_space = features_categorical + features_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'rf': LGBMClassifier(boosting_type='rf', bagging_freq=5, bagging_fraction=.05, feature_fraction=.1),\n",
    "    'lr': LogisticRegression(random_state=42, multi_class='ovr'),\n",
    "    'svc': LinearSVC(multi_class='ovr'),\n",
    "}\n",
    "\n",
    "def score_models(models, X, y, folds = 8):\n",
    "    stats = {}\n",
    "\n",
    "    for k, model in models.items():\n",
    "        stats[k] = []\n",
    "        kfold = KFold(n_splits=8, shuffle=True)\n",
    "\n",
    "        for train_ix, test_ix in kfold.split(X, y):\n",
    "            X_crossval_train, X_crossval_test = X[train_ix], X[test_ix]\n",
    "            y_crossval_train, y_crossval_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            # here must be sume sort of optimization\n",
    "            model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "            stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))\n",
    "    \n",
    "    for model, model_stats in stats.items():\n",
    "        print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 1, 6, 11, 34, 12, 25, 48, 18, 44, 3, 5, 41, 49, 24, 37, 4, 14, 42, 31, 29, 36, 23, 28, 43, 22, 30, 38, 32, 7, 10, 20, 33, 40, 27, 45, 8, 17, 9, 16, 46, 15, 47, 13, 19, 26, 35, 39, 21]\n",
      "score on new features:\n",
      "rf, Accuracy: 0.5729566371167312\n",
      "lr, Accuracy: 0.6778919626145108\n",
      "svc, Accuracy: 0.6784047437688753\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "global_stats = []\n",
    "ix_subsets = []\n",
    "\n",
    "initial_feature = 0# np.random.randint(0, len(feature_space))\n",
    "\n",
    "dataset_size = X.shape[0]\n",
    "# free_features is a list of observation indices\n",
    "free_features = [i for i in range(len(feature_space)) if i != initial_feature]\n",
    "# subset is a list of observations\n",
    "subset = [feature_space[initial_feature]['onehot']]\n",
    "# indices of the features in the subset\n",
    "subset_indices = [initial_feature]\n",
    "\n",
    "while len(subset) != len(feature_space):\n",
    "    max_entropy = -1\n",
    "    feature_index = -1\n",
    "    new_feature = None\n",
    "\n",
    "    local_stats = []\n",
    "\n",
    "\n",
    "    for ix_feature, feature in enumerate(free_features):\n",
    "        if len(subset) > 1:\n",
    "            input_features = np.concatenate(subset, axis=1)\n",
    "        else:\n",
    "            input_features = subset[0]\n",
    "\n",
    "        clr = copy(models['lr'])\n",
    "        #train model on the given subset and new feature\n",
    "        clr.fit(input_features, feature_space[feature]['category'].squeeze())\n",
    "        #predict that feature with the given subset\n",
    "        predicted = clr.predict(input_features)\n",
    "\n",
    "        predicted_onehot = encoders[feature].transform(predicted.reshape(-1, 1))\n",
    "        pred_difference = (predicted_onehot != feature_space[feature]['onehot']).astype(np.int32) # 0110\n",
    "        #pred_difference = ((predicted_onehot == 1) & (feature_space[feature]['onehot'] == 0)).astype(np.int32) # 0010\n",
    "\n",
    "        pred_category, pred_counts = np.unique(predicted, return_counts=True)\n",
    "        real_category, real_counts = np.unique(feature_space[feature]['category'], return_counts=True)\n",
    "\n",
    "        pred_proba = pred_counts / dataset_size\n",
    "        real_proba = real_counts / dataset_size\n",
    "\n",
    "        real_stats = dict(zip(real_category, real_proba))\n",
    "        pred_stats =  dict(zip(pred_category, pred_proba))\n",
    "\n",
    "        synchronize_counts(real_stats, pred_stats)\n",
    "\n",
    "        ce_r_r = cross_entropy(list(real_stats.values()), list(real_stats.values()))\n",
    "        ce_p_p = cross_entropy(list(pred_stats.values()), list(pred_stats.values()))\n",
    "        ce_r_p = cross_entropy(list(real_stats.values()), list(pred_stats.values()))\n",
    "\n",
    "        if max_entropy < ce_r_p:\n",
    "            max_entropy = ce_r_p\n",
    "            feature_index = ix_feature\n",
    "            new_feature = pred_difference\n",
    "\n",
    "        local_stats.append({'rr': ce_r_r, 'pp': ce_p_p, 'rp': ce_r_p})\n",
    "    #print('#'*100)\n",
    "    #print('subset', subset)\n",
    "    #print(pd.DataFrame(stats, index=free_features))\n",
    "    #print('#'*100)\n",
    "\n",
    "    subset.append(new_feature)\n",
    "    subset_indices.append(free_features[feature_index])\n",
    "    del free_features[feature_index]\n",
    "\n",
    "#print('generated ordered list of variables')\n",
    "global_stats.append(local_stats)\n",
    "ix_subsets.append(subset_indices)\n",
    "print(subset_indices)\n",
    "print('score on new features:')\n",
    "score_models(models, np.hstack(subset), y, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parkinsons Telemonitoring Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/parkinsons_updrs.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (5875, 22)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = MedianBinarizer(n_bins=32).fit(df.total_UPDRS.values.reshape(-1, 1)).transform_categorical(df.total_UPDRS.values.reshape(-1, 1))\n",
    "df.drop(['total_UPDRS'], axis=1, inplace=True)\n",
    "\n",
    "categorical_features = ['subject#']\n",
    "\n",
    "\n",
    "features_categorical = list()\n",
    "features_real = list()\n",
    "encoders = list()\n",
    "\n",
    "for i in categorical_features:\n",
    "    encoder = OneHotEncoder().fit(df[i].values.reshape(-1, 1))\n",
    "    \n",
    "    onehot_repr = encoder.transform(df[i].values.reshape(-1, 1)).todense()\n",
    "    categorical_repr = df[i].values.reshape(-1, 1)\n",
    "    \n",
    "    if np.unique(categorical_repr).size < 2:\n",
    "        print('feature {} is ignored'.format(i))\n",
    "        continue\n",
    "        \n",
    "    encoders.append(encoder)\n",
    "    \n",
    "    features_categorical.append({\n",
    "        'onehot': onehot_repr,\n",
    "        'category': categorical_repr\n",
    "    })\n",
    "\n",
    "for i in np.setdiff1d(ar1=df.columns, ar2=categorical_features):\n",
    "    binarizer = MedianBinarizer(n_bins=32).fit(df[i].values.reshape(-1, 1))\n",
    "    \n",
    "    onehot_repr = binarizer.transform_onehot(df[i].values.reshape(-1, 1)),\n",
    "    categorical_repr = binarizer.transform_categorical(df[i].values.reshape(-1, 1))\n",
    "    \n",
    "    if np.unique(categorical_repr).size < 2:\n",
    "        print('feature {} is ignored'.format(i))\n",
    "        continue\n",
    "        \n",
    "    encoders.append(binarizer.encoder)\n",
    "    \n",
    "    features_real.append({\n",
    "        'onehot': binarizer.transform_onehot(df[i].values.reshape(-1, 1)),\n",
    "        'category': binarizer.transform_categorical(df[i].values.reshape(-1, 1))\n",
    "    })\n",
    "\n",
    "feature_space = features_categorical + features_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 20, 6, 4, 9, 18, 13, 7, 8, 10, 11, 5, 12, 2, 17, 15, 14, 16, 1, 19]\n",
      "score on new features:\n",
      "rf, Accuracy: 0.4090235685554876\n",
      "lr, Accuracy: 0.8280848579213702\n",
      "svc, Accuracy: 0.8326769263563736\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "global_stats = []\n",
    "ix_subsets = []\n",
    "\n",
    "initial_feature = 0# np.random.randint(0, len(feature_space))\n",
    "\n",
    "dataset_size = X.shape[0]\n",
    "# free_features is a list of observation indices\n",
    "free_features = [i for i in range(len(feature_space)) if i != initial_feature]\n",
    "# subset is a list of observations\n",
    "subset = [feature_space[initial_feature]['onehot']]\n",
    "# indices of the features in the subset\n",
    "subset_indices = [initial_feature]\n",
    "\n",
    "while len(subset) != len(feature_space):\n",
    "    max_entropy = -1\n",
    "    feature_index = -1\n",
    "    new_feature = None\n",
    "\n",
    "    local_stats = []\n",
    "\n",
    "\n",
    "    for ix_feature, feature in enumerate(free_features):\n",
    "        if len(subset) > 1:\n",
    "            input_features = np.concatenate(subset, axis=1)\n",
    "        else:\n",
    "            input_features = subset[0]\n",
    "\n",
    "        clr = copy(models['lr'])\n",
    "        #train model on the given subset and new feature\n",
    "        clr.fit(input_features, feature_space[feature]['category'].squeeze())\n",
    "        #predict that feature with the given subset\n",
    "        predicted = clr.predict(input_features)\n",
    "\n",
    "        predicted_onehot = encoders[feature].transform(predicted.reshape(-1, 1))\n",
    "        pred_difference = (predicted_onehot != feature_space[feature]['onehot']).astype(np.int32) # 0110\n",
    "        #pred_difference = ((predicted_onehot == 1) & (feature_space[feature]['onehot'] == 0)).astype(np.int32) # 0010\n",
    "\n",
    "        pred_category, pred_counts = np.unique(predicted, return_counts=True)\n",
    "        real_category, real_counts = np.unique(feature_space[feature]['category'], return_counts=True)\n",
    "\n",
    "        pred_proba = pred_counts / dataset_size\n",
    "        real_proba = real_counts / dataset_size\n",
    "\n",
    "        real_stats = dict(zip(real_category, real_proba))\n",
    "        pred_stats =  dict(zip(pred_category, pred_proba))\n",
    "\n",
    "        synchronize_counts(real_stats, pred_stats)\n",
    "\n",
    "        ce_r_r = cross_entropy(list(real_stats.values()), list(real_stats.values()))\n",
    "        ce_p_p = cross_entropy(list(pred_stats.values()), list(pred_stats.values()))\n",
    "        ce_r_p = cross_entropy(list(real_stats.values()), list(pred_stats.values()))\n",
    "\n",
    "        if max_entropy < ce_r_p:\n",
    "            max_entropy = ce_r_p\n",
    "            feature_index = ix_feature\n",
    "            new_feature = pred_difference\n",
    "\n",
    "        local_stats.append({'rr': ce_r_r, 'pp': ce_p_p, 'rp': ce_r_p})\n",
    "    #print('#'*100)\n",
    "    #print('subset', subset)\n",
    "    #print(pd.DataFrame(stats, index=free_features))\n",
    "    #print('#'*100)\n",
    "\n",
    "    subset.append(new_feature)\n",
    "    subset_indices.append(free_features[feature_index])\n",
    "    del free_features[feature_index]\n",
    "\n",
    "#print('generated ordered list of variables')\n",
    "global_stats.append(local_stats)\n",
    "ix_subsets.append(subset_indices)\n",
    "print(subset_indices)\n",
    "print('score on new features:')\n",
    "score_models(models, np.hstack(subset), y, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy efficiency Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../datasets/ENB2012_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (768, 10)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature X3 is ignored\n",
      "feature X5 is ignored\n"
     ]
    }
   ],
   "source": [
    "y = MedianBinarizer(n_bins=32).fit(df.Y1.values.reshape(-1, 1)).transform_categorical(df.Y1.values.reshape(-1, 1))\n",
    "df.drop(['Y1', 'Y2'], axis=1, inplace=True)\n",
    "\n",
    "categorical_features = ['X6']\n",
    "\n",
    "\n",
    "features_categorical = list()\n",
    "features_real = list()\n",
    "encoders = list()\n",
    "\n",
    "for i in categorical_features:\n",
    "    encoder = OneHotEncoder().fit(df[i].values.reshape(-1, 1))\n",
    "    \n",
    "    onehot_repr = encoder.transform(df[i].values.reshape(-1, 1)).todense()\n",
    "    categorical_repr = df[i].values.reshape(-1, 1)\n",
    "    \n",
    "    if np.unique(categorical_repr).size < 2:\n",
    "        print('feature {} is ignored'.format(i))\n",
    "        continue\n",
    "        \n",
    "    encoders.append(encoder)\n",
    "    \n",
    "    features_categorical.append({\n",
    "        'onehot': onehot_repr,\n",
    "        'category': categorical_repr\n",
    "    })\n",
    "\n",
    "for i in np.setdiff1d(ar1=df.columns, ar2=categorical_features):\n",
    "    binarizer = MedianBinarizer(n_bins=32).fit(df[i].values.reshape(-1, 1))\n",
    "    \n",
    "    onehot_repr = binarizer.transform_onehot(df[i].values.reshape(-1, 1)),\n",
    "    categorical_repr = binarizer.transform_categorical(df[i].values.reshape(-1, 1))\n",
    "    \n",
    "    if np.unique(categorical_repr).size < 2:\n",
    "        print('feature {} is ignored'.format(i))\n",
    "        continue\n",
    "        \n",
    "    encoders.append(binarizer.encoder)\n",
    "    \n",
    "    features_real.append({\n",
    "        'onehot': binarizer.transform_onehot(df[i].values.reshape(-1, 1)),\n",
    "        'category': binarizer.transform_categorical(df[i].values.reshape(-1, 1))\n",
    "    })\n",
    "\n",
    "feature_space = features_categorical + features_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 3, 4, 2, 1]\n",
      "score on new features:\n",
      "rf, Accuracy: 0.54296875\n",
      "lr, Accuracy: 0.7604166666666666\n",
      "svc, Accuracy: 0.7591145833333334\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "global_stats = []\n",
    "ix_subsets = []\n",
    "\n",
    "initial_feature = 0# np.random.randint(0, len(feature_space))\n",
    "\n",
    "dataset_size = X.shape[0]\n",
    "# free_features is a list of observation indices\n",
    "free_features = [i for i in range(len(feature_space)) if i != initial_feature]\n",
    "# subset is a list of observations\n",
    "subset = [feature_space[initial_feature]['onehot']]\n",
    "# indices of the features in the subset\n",
    "subset_indices = [initial_feature]\n",
    "\n",
    "while len(subset) != len(feature_space):\n",
    "    max_entropy = -1\n",
    "    feature_index = -1\n",
    "    new_feature = None\n",
    "\n",
    "    local_stats = []\n",
    "\n",
    "\n",
    "    for ix_feature, feature in enumerate(free_features):\n",
    "        if len(subset) > 1:\n",
    "            input_features = np.concatenate(subset, axis=1)\n",
    "        else:\n",
    "            input_features = subset[0]\n",
    "\n",
    "        clr = copy(models['lr'])\n",
    "        #train model on the given subset and new feature\n",
    "        clr.fit(input_features, feature_space[feature]['category'].squeeze())\n",
    "        #predict that feature with the given subset\n",
    "        predicted = clr.predict(input_features)\n",
    "\n",
    "        predicted_onehot = encoders[feature].transform(predicted.reshape(-1, 1))\n",
    "        pred_difference = (predicted_onehot != feature_space[feature]['onehot']).astype(np.int32) # 0110\n",
    "        #pred_difference = ((predicted_onehot == 1) & (feature_space[feature]['onehot'] == 0)).astype(np.int32) # 0010\n",
    "\n",
    "        pred_category, pred_counts = np.unique(predicted, return_counts=True)\n",
    "        real_category, real_counts = np.unique(feature_space[feature]['category'], return_counts=True)\n",
    "\n",
    "        pred_proba = pred_counts / dataset_size\n",
    "        real_proba = real_counts / dataset_size\n",
    "\n",
    "        real_stats = dict(zip(real_category, real_proba))\n",
    "        pred_stats =  dict(zip(pred_category, pred_proba))\n",
    "\n",
    "        synchronize_counts(real_stats, pred_stats)\n",
    "\n",
    "        ce_r_r = cross_entropy(list(real_stats.values()), list(real_stats.values()))\n",
    "        ce_p_p = cross_entropy(list(pred_stats.values()), list(pred_stats.values()))\n",
    "        ce_r_p = cross_entropy(list(real_stats.values()), list(pred_stats.values()))\n",
    "\n",
    "        if max_entropy < ce_r_p:\n",
    "            max_entropy = ce_r_p\n",
    "            feature_index = ix_feature\n",
    "            new_feature = pred_difference\n",
    "\n",
    "        local_stats.append({'rr': ce_r_r, 'pp': ce_p_p, 'rp': ce_r_p})\n",
    "    #print('#'*100)\n",
    "    #print('subset', subset)\n",
    "    #print(pd.DataFrame(stats, index=free_features))\n",
    "    #print('#'*100)\n",
    "\n",
    "    subset.append(new_feature)\n",
    "    subset_indices.append(free_features[feature_index])\n",
    "    del free_features[feature_index]\n",
    "\n",
    "#print('generated ordered list of variables')\n",
    "global_stats.append(local_stats)\n",
    "ix_subsets.append(subset_indices)\n",
    "print(subset_indices)\n",
    "print('score on new features:')\n",
    "score_models(models, np.hstack(subset), y, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
