{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array, as_float_array\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector, ExhaustiveFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import mlrank.hyperparams_opt as hyperparams_opt\n",
    "\n",
    "reload(hyperparams_opt)\n",
    "\n",
    "bayesian_optimization_lightgbm = hyperparams_opt.bayesian_optimization_lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_holdout_interations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./classification/datasets/breast_cancer.csv')\n",
    "y = df.diagnosis.replace('M', 0).replace('B', 1).values\n",
    "X = np.asarray(df.drop(['diagnosis', 'id', 'Unnamed: 32'], axis=1).as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X, y, cv=4, max_iter_opt=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'rf': LGBMClassifier(boosting_type='rf', min_child_samples=10, **params_opt),\n",
    "    'lr': LogisticRegression(random_state=42, multi_class='ovr', solver='liblinear', C=10000, tol=1e-2),\n",
    "    'svc': LinearSVC(multi_class='ovr', C=10000, tol=1e-2, max_iter=250),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SequentialFeatureSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9263859649122808\n",
      "0.9604561403508773\n",
      "0.9388070175438595\n"
     ]
    }
   ],
   "source": [
    "accur_score = list()\n",
    "\n",
    "for i in range(n_holdout_interations):\n",
    "    record = dict()\n",
    "    for name, model in models.items():\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5)#, random_state=1)\n",
    "        \n",
    "        sfs = SequentialFeatureSelector(model, k_features=5, scoring='accuracy')\n",
    "        sfs.fit(X_train, y_train)\n",
    "        \n",
    "        model.fit(X_train[:, [int(i) for i in sfs.subsets_[5]['feature_names']]], y_train.squeeze())\n",
    "        record[name] = accuracy_score(model.predict(X_val[:, [int(i) for i in sfs.subsets_[5]['feature_names']]]), y_val.squeeze())\n",
    "    accur_score.append(record)\n",
    "\n",
    "print(np.mean([i['svc'] for i in accur_score]))\n",
    "print(np.mean([i['lr'] for i in accur_score]))\n",
    "print(np.mean([i['rf'] for i in accur_score]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExhaustiveFeatureSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExhaustiveFeatureSelector(clone_estimator=True, cv=5,\n",
       "             estimator=LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='liblinear',\n",
       "          tol=0.01, verbose=0, warm_start=False),\n",
       "             max_features=5, min_features=1, n_jobs=-1,\n",
       "             pre_dispatch='2*n_jobs', print_progress=False,\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efs = ExhaustiveFeatureSelector(estimator=models['lr'], max_features=5, scoring='accuracy', n_jobs=-1, print_progress=False)\n",
    "efs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('17', '21', '23', '24', '28')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efs.best_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9732280701754387\n"
     ]
    }
   ],
   "source": [
    "accur_score = list()\n",
    "\n",
    "for i in range(n_holdout_interations):\n",
    "    record = dict()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X[:, [int(i) for i in efs.best_feature_names_]], y, test_size=0.5)#, random_state=1)\n",
    "        \n",
    "    models['lr'].fit(X_train, y_train.squeeze())\n",
    "    record['lr'] = accuracy_score(models['lr'].predict(X_val), y_val.squeeze())\n",
    "    accur_score.append(record)\n",
    "\n",
    "print(np.mean([i['lr'] for i in accur_score]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecursiveFeatureElimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feats = dict()\n",
    "\n",
    "for k, v in models.items():\n",
    "    model_feats[k] = [int(i) for i in sfs.subsets_[5]['feature_names']]\n",
    "    rfe = RFE(estimator=v, n_features_to_select=5)\n",
    "    rfe.fit(X, y)\n",
    "    model_feats[k] = np.array(list(range(X.shape[1])))[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': array([10, 20, 21, 23, 27]),\n",
       " 'rf': array([ 9, 15, 16, 17, 19]),\n",
       " 'svc': array([ 0,  2,  7, 13, 23])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082105263157895\n",
      "0.9637543859649124\n",
      "0.6942456140350877\n"
     ]
    }
   ],
   "source": [
    "accur_score = list()\n",
    "\n",
    "for i in range(n_holdout_interations):\n",
    "    record = dict()\n",
    "    for name, model in models.items():\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X[:, model_feats[name]], y, test_size=0.5)#, random_state=1)\n",
    "        \n",
    "        model.fit(X_train, y_train.squeeze())\n",
    "        record[name] = accuracy_score(model.predict(X_val), y_val.squeeze())\n",
    "    accur_score.append(record)\n",
    "\n",
    "print(np.mean([i['svc'] for i in accur_score]))\n",
    "print(np.mean([i['lr'] for i in accur_score]))\n",
    "print(np.mean([i['rf'] for i in accur_score]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression coefficents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coefs = []\n",
    "\n",
    "c = copy(models['lr'])\n",
    "c.fit(X, y)\n",
    "    \n",
    "model_coefs = np.abs(c.coef_).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.8956650592717343, 10),\n",
       " (1.7403622083957953, 21),\n",
       " (1.3205567864813037, 20),\n",
       " (1.3197392480935997, 28),\n",
       " (1.2445662612696022, 13)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(model_coefs.tolist(), range(model_coefs.shape[0])), key = lambda x: -x[0])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9244210526315788\n",
      "0.9571578947368421\n",
      "0.9078947368421052\n"
     ]
    }
   ],
   "source": [
    "accur_score = list()\n",
    "\n",
    "for i in range(n_holdout_interations):\n",
    "    record = dict()\n",
    "    for name, model in models.items():\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X[:, [10, 21, 20, 28, 13]], y, test_size=0.5)#, random_state=1)\n",
    "        \n",
    "        model.fit(X_train, y_train.squeeze())\n",
    "        record[name] = accuracy_score(model.predict(X_val), y_val.squeeze())\n",
    "    accur_score.append(record)\n",
    "\n",
    "print(np.mean([i['svc'] for i in accur_score]))\n",
    "print(np.mean([i['lr'] for i in accur_score]))\n",
    "print(np.mean([i['rf'] for i in accur_score]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37 41 15 46 37 10  9 25 83 24 19 13  4 57 45 23 73 31  0 38 24 25 17  8\n",
      " 49 18 31 32 27 51]\n",
      "[ 8 16 13 29 24  3 14  1 19  0  4 27 17 26 28  7 21 20  9 15 10 25 22  2\n",
      " 11  5  6 23 12 18]\n"
     ]
    }
   ],
   "source": [
    "model_coefs = []\n",
    "\n",
    "c = copy(models['rf'])\n",
    "c.fit(X, y)\n",
    "    \n",
    "print(c.feature_importances_)\n",
    "print(np.argsort(c.feature_importances_)[::-1])#[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8868070175438596\n",
      "0.9376842105263159\n",
      "0.901017543859649\n"
     ]
    }
   ],
   "source": [
    "accur_score = list()\n",
    "\n",
    "for i in range(n_holdout_interations):\n",
    "    record = dict()\n",
    "    for name, model in models.items():\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X[:, [8, 16, 13,  3, 24]], y, test_size=0.5)#, random_state=1)\n",
    "        \n",
    "        model.fit(X_train, y_train.squeeze())\n",
    "        record[name] = accuracy_score(model.predict(X_val), y_val.squeeze())\n",
    "    accur_score.append(record)\n",
    "\n",
    "print(np.mean([i['svc'] for i in accur_score]))\n",
    "print(np.mean([i['lr'] for i in accur_score]))\n",
    "print(np.mean([i['rf'] for i in accur_score]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
