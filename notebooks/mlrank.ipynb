{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array, as_float_array\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxentropyMedianDichtomizationTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_splits, verbose=False):\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "        self.n_samples = None\n",
    "        self.n_features = None\n",
    "        self._splits = None\n",
    "        self._splits_indices = None\n",
    "    \n",
    "    def _check_X(self, X, n_features=None):\n",
    "        _X = None\n",
    "        if not hasattr(X, 'dtype'):\n",
    "            _X = check_array(as_float_array(X))\n",
    "        _X = check_array(X)\n",
    "        \n",
    "        if self.n_features:\n",
    "            if _X.shape[1] != self.n_features:\n",
    "                raise Exception('X has {} columns while {} are expected'.format(_X.shape[1], self.n_features))\n",
    "        return _X\n",
    "    \n",
    "    def _calc_entropy(self, X, split_bias):\n",
    "        a = np.sum(X < split_bias)\n",
    "        b = np.sum(X >= split_bias)\n",
    "\n",
    "        p = np.array([a / X.shape[0], b / X.shape[0]])\n",
    "        return -np.sum(np.log(p + 1) * p)\n",
    "\n",
    "\n",
    "    def _get_maxentropy_split(self, X):\n",
    "        # O(n^2)\n",
    "        block_size = X.shape[0]\n",
    "        ix_max_entropy = -1\n",
    "        X_diff = np.diff(X)\n",
    "\n",
    "        start_point = X.shape[0] // 2\n",
    "        \n",
    "        right_point = right_entropy = None\n",
    "        left_point = left_entropy =None\n",
    "        \n",
    "        max_entropy = -1\n",
    "        max_probas = None\n",
    "        # define point where to start looking for\n",
    "        # highest entropy\n",
    "        if X_diff[start_point] == 0:\n",
    "            _right_indices = np.where(X_diff[start_point:] > 0)[0]\n",
    "            _left_indices = np.where(X_diff[:start_point] > 0)[0]\n",
    "            \n",
    "            if _right_indices.any():\n",
    "                right_point = _right_indices[0] + start_point\n",
    "            \n",
    "            if _left_indices.any():\n",
    "                left_point = _left_indices[-1]\n",
    "            \n",
    "            # if we have constant series\n",
    "            if _right_indices is None and _left_indices is None:\n",
    "                return 0, -1\n",
    "            \n",
    "            if right_point:\n",
    "                right_entropy = self._calc_entropy(X, X[right_point])\n",
    "        \n",
    "            if left_point:\n",
    "                left_entropy = self._calc_entropy(X, X[left_point])\n",
    "        else:\n",
    "            right_point = start_point + 1\n",
    "            left_point = start_point - 1\n",
    "            \n",
    "            right_entropy = self._calc_entropy(X, X[right_point])\n",
    "            left_entropy = self._calc_entropy(X, X[left_point])\n",
    "            center_entropy = self._calc_entropy(X, X[start_point])\n",
    "            \n",
    "            if center_entropy > left_entropy and center_entropy > right_entropy:\n",
    "                return center_entropy, start_point\n",
    "\n",
    "        # if entropy at the point left to the starting point is higher\n",
    "        # search for entropy maxima\n",
    "        if right_point and  (not left_point or right_entropy > left_entropy):\n",
    "            for j in range(right_point + 1, block_size):\n",
    "                local_entropy = self._calc_entropy(X, X[j])\n",
    "                if local_entropy > right_entropy:\n",
    "                    right_point = j\n",
    "                    right_entropy = local_entropy\n",
    "                else:\n",
    "                    return right_entropy, right_point\n",
    "        elif left_point:\n",
    "            for j in reversed(range(0, left_point - 1)):\n",
    "                local_entropy = self._calc_entropy(X, X[j])\n",
    "                if local_entropy > left_entropy:\n",
    "                    left_point = j\n",
    "                    left_entropy = local_entropy\n",
    "                else:\n",
    "                    return left_entropy, left_point\n",
    "        \n",
    "        return 0, -1\n",
    "\n",
    "    def _dichtomize(self, X):\n",
    "        # O(n^2 * log n)\n",
    "        \n",
    "        _iters = np.log2(self.n_splits)\n",
    "        if _iters - int(_iters) != 0:\n",
    "            raise Exception('number of bins should be of a power of 2')\n",
    "        \n",
    "        # make first maxentropy split\n",
    "        _, initial_bin = self._get_maxentropy_split(X)\n",
    "        splits_current_feature = [(0, initial_bin), (initial_bin, self.n_samples - 1)]\n",
    "        for i in range(int(_iters) - 1):\n",
    "            # an empty list for splits in current iteration\n",
    "            _splits = list()\n",
    "            for j in splits_current_feature:\n",
    "                entropy, index = self._get_maxentropy_split(X[j[0]: j[1]])\n",
    "                if entropy == 0:\n",
    "                    _splits += [(j[0], j[1])]\n",
    "                else:\n",
    "                    _splits += [(j[0], j[0] + index), (j[0] + index, j[1])]\n",
    "\n",
    "            splits_current_feature = _splits\n",
    "            \n",
    "        return splits_current_feature\n",
    "    \n",
    "    def _convert(self, X, ix):\n",
    "        result = list()\n",
    "        for x in X.flatten():\n",
    "            result.append(np.argwhere([k[0] <= x and x < k[1] for k in self._splits[ix]]))\n",
    "        return np.array(result).reshape(-1, 1) \n",
    "    \n",
    "    def fit(self, X):\n",
    "        X = self._check_X(X)\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        self._splits = list()\n",
    "        self._splits_indices = list()\n",
    "        \n",
    "        for ix in range(self.n_features):\n",
    "            x = np.sort(X[:, ix].flatten())\n",
    "            _indices = self._dichtomize(x.flatten())\n",
    "            \n",
    "            self._splits_indices.append(_indices)\n",
    "            self._splits.append([[x[i[0]], x[i[1]]] for i in _indices])\n",
    "            \n",
    "            self._splits[-1][0][0] = -np.inf\n",
    "            self._splits[-1][-1][1] = np.inf\n",
    "            \n",
    "            self._splits = np.array(self._splits)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        _, n_features = X.shape\n",
    "        X = self._check_X(X, n_features)\n",
    "        \n",
    "        X_categorical = list()\n",
    "        for ix in range(n_features):\n",
    "            X_categorical.append(self._convert(X, ix))\n",
    "            \n",
    "        return np.hstack(X_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeFabulousTransformation(BaseEstimator):\n",
    "    def __init__(self, base_estimator, dichtomized=False, n_splits=32, exhausitve=True, random_seed=42, verbose=1):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_splits = n_splits\n",
    "        self.random_seed = random_seed\n",
    "        self.exhausitve = exhausitve\n",
    "        self.verbose = verbose\n",
    "        self.dichtomized=dichtomized\n",
    "        self._feature_space = None\n",
    "        self._feature_dichtomizers = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _cross_entropy(p, q):\n",
    "        p = np.array(p)\n",
    "        q = np.array(q)\n",
    "        q[q == 0] = 1e-8\n",
    "        return -np.sum(p * np.log(q))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _synchronize_two_dicts(_from:dict, _to:dict):\n",
    "        for i in np.setdiff1d(list(_from.keys()), list(_to.keys())):\n",
    "            _to[i] = 0\n",
    "            \n",
    "    def _dichtomize(self, X):\n",
    "        self._feature_dichtomizers = list()\n",
    "        self._feature_space = list()\n",
    "        \n",
    "        if not self.dichtomized:\n",
    "            for i in range(X.shape[1]):\n",
    "                feature = X[:, i].reshape(-1, 1)\n",
    "                dichtomizer = MaxentropyMedianDichtomizationTransformer(32).fit(feature)\n",
    "                feature_dichtomized = dichtomizer.transform(feature)\n",
    "                onehot_encoder = OneHotEncoder(sparse=True).fit(feature_dichtomized)\n",
    "\n",
    "                self._feature_dichtomizers.append({'dichtomizer': dichtomizer, 'encoder': onehot_encoder})\n",
    "                self._feature_space.append({'categorical': feature_dichtomized, 'binary': onehot_encoder.transform(feature_dichtomized)})\n",
    "        else:\n",
    "            for i in range(X.shape[1]):\n",
    "                feature = X[:, i].reshape(-1, 1)\n",
    "                onehot_encoder = OneHotEncoder(sparse=True).fit(feature)\n",
    "                self._feature_dichtomizers.append({'dichtomizer': None, 'encoder': onehot_encoder})\n",
    "                self._feature_space.append({'categorical': feature, 'binary': onehot_encoder.transform(feature)})\n",
    "                \n",
    "    \n",
    "    def _calc_cross_entropy(self, ix_feature, pred, dataset_size):\n",
    "        real = np.asarray(self._feature_space[ix_feature]['binary'].sum(1))\n",
    "        pred = np.asarray(pred.sum(1))\n",
    "        \n",
    "        # TODO: уточнить \n",
    "        pred[np.argwhere(pred == 2)] = 1\n",
    "        \n",
    "        pred_category, pred_counts = np.unique(pred, return_counts=True)\n",
    "        real_category, real_counts = np.unique(real, return_counts=True)\n",
    "\n",
    "        pred_proba = pred_counts / dataset_size\n",
    "        real_proba = real_counts / dataset_size\n",
    "\n",
    "        real_stats = dict(zip(real_category, real_proba))\n",
    "        pred_stats =  dict(zip(pred_category, pred_proba))\n",
    "\n",
    "        SomeFabulousTransformation._synchronize_two_dicts(real_stats, pred_stats)\n",
    "        \n",
    "        return SomeFabulousTransformation._cross_entropy(list(real_stats.values()), list(pred_stats.values()))\n",
    "    \n",
    "    def _fit_transform(self, X, initial_feature_ix):\n",
    "        dataset_size = X.shape[0]\n",
    "        free_features_ix = [i for i in range(len(self._feature_space)) if i != initial_feature_ix]\n",
    "        active_features_subset = [self._feature_space[initial_feature_ix]['binary']]\n",
    "        active_features_subset_ix = [initial_feature_ix]\n",
    "        \n",
    "        while len(active_features_subset) != len(self._feature_space):\n",
    "            max_entropy = -1\n",
    "            max_entropy_feature_ix = -1\n",
    "            max_entropy_feature_value = None\n",
    "            \n",
    "            if self.verbose > 1:\n",
    "                print('currently processed {} features out of {}'.format(len(active_features_subset), len(self._feature_space)))\n",
    "                print('number of active features {}'.format(len(active_features_subset)))\n",
    "            \n",
    "            for ix_current_feature in free_features_ix:\n",
    "                if len(active_features_subset) > 1:\n",
    "                    model_input_features = sparse.hstack(active_features_subset)\n",
    "                else:\n",
    "                    model_input_features = active_features_subset[0]\n",
    "                \n",
    "                estimator = copy(self.base_estimator)\n",
    "                estimator.fit(model_input_features, self._feature_space[ix_current_feature]['categorical'].squeeze())\n",
    "                \n",
    "                pred = estimator.predict(model_input_features)\n",
    "\n",
    "                pred_onehot = self._feature_dichtomizers[ix_current_feature]['encoder'].transform(pred.reshape(-1, 1))\n",
    "                pred_diff = (pred_onehot != self._feature_space[ix_current_feature]['binary']).astype(np.int32)\n",
    "                \n",
    "                entropy = self._calc_cross_entropy(ix_current_feature, pred_diff, dataset_size)\n",
    "                \n",
    "                if entropy > max_entropy:\n",
    "                    max_entropy_feature_value = pred_diff\n",
    "                    max_entropy_feature_ix = ix_current_feature\n",
    "                    max_entropy = entropy\n",
    "            \n",
    "            free_features_ix.remove(max_entropy_feature_ix)\n",
    "            active_features_subset.append(max_entropy_feature_value)\n",
    "            active_features_subset_ix.append(max_entropy_feature_ix)\n",
    "        \n",
    "        return np.hstack(active_features_subset), active_features_subset_ix      \n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        self._dichtomize(X)\n",
    "        \n",
    "        if not self.exhausitve:\n",
    "            initial_feature_ix = np.random.randint(0, len(self._feature_space))\n",
    "            return self._fit_transform(X, initial_feature_ix)\n",
    "        \n",
    "        features_subset = list()\n",
    "        features_subset_ix = list()\n",
    "        for initial_feature_ix in range(len(self._feature_space)):\n",
    "            if self.verbose == 1:\n",
    "                print('processing starting feature {}'.format(initial_feature_ix))\n",
    "            _features_subset, _features_subset_ix = self._fit_transform(X, initial_feature_ix)\n",
    "            \n",
    "            features_subset.append(_features_subset)\n",
    "            features_subset_ix.append(_features_subset_ix)\n",
    "            \n",
    "        return features_subset, np.vstack(features_subset_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперимент на синтетике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coefs = make_regression(n_samples=1000, n_features=15, n_informative=5, n_targets=4, coef=True, random_state=42)\n",
    "X = np.concatenate([X, y[:, :-1]], axis=1)\n",
    "y = y[:, -1]#.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'rf': LGBMClassifier(boosting_type='rf', bagging_freq=5, bagging_fraction=.05, feature_fraction=.1),\n",
    "    'lr': LogisticRegression(random_state=42, multi_class='ovr', solver='liblinear', C=10000, tol=1e-2, n_jobs=10),\n",
    "    'svc': LinearSVC(multi_class='ovr'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = SomeFabulousTransformation(models['lr'], exhausitve=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing starting feature 0\n",
      "processing starting feature 1\n",
      "processing starting feature 2\n",
      "processing starting feature 3\n",
      "processing starting feature 4\n",
      "processing starting feature 5\n",
      "processing starting feature 6\n",
      "processing starting feature 7\n",
      "processing starting feature 8\n",
      "processing starting feature 9\n",
      "processing starting feature 10\n",
      "processing starting feature 11\n",
      "processing starting feature 12\n",
      "processing starting feature 13\n",
      "processing starting feature 14\n",
      "processing starting feature 15\n",
      "processing starting feature 16\n",
      "processing starting feature 17\n"
     ]
    }
   ],
   "source": [
    "features, indices = transform.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  5, 12,  2, 16, 17,  8, 15,  1, 10, 13, 14,  4,  6, 11,  3,\n",
       "         9,  7],\n",
       "       [ 1,  2, 13,  0, 16, 17, 12,  9,  4, 10,  5, 11,  3, 15,  6,  7,\n",
       "        14,  8],\n",
       "       [ 2,  1, 17,  4, 16,  0,  7,  6, 10, 11, 14, 13,  8,  3,  5,  9,\n",
       "        12, 15],\n",
       "       [ 3, 17, 14,  2, 16, 11, 10,  9,  4,  1,  7,  6,  0,  5, 15,  8,\n",
       "        13, 12],\n",
       "       [ 4,  2,  5, 11, 17,  1, 16,  0,  9, 14,  3, 15, 10,  7,  8,  6,\n",
       "        12, 13],\n",
       "       [ 5,  2,  6, 11, 16, 14, 17,  3, 12,  7,  9,  1,  4,  8,  0, 13,\n",
       "        10, 15],\n",
       "       [ 6,  1,  9,  0, 17, 10, 14,  3,  5,  8, 12,  4,  2, 13,  7, 11,\n",
       "        15, 16],\n",
       "       [ 7,  9, 16,  3,  2, 13,  4, 14, 15,  1,  0,  6, 10, 11,  8, 17,\n",
       "        12,  5],\n",
       "       [ 8,  0,  2,  5, 16,  6, 11,  4,  9, 17, 13, 15, 12, 10,  7,  3,\n",
       "         1, 14],\n",
       "       [ 9,  7, 16, 10, 12, 15,  5,  2, 14, 11,  4,  6,  0,  3,  8, 13,\n",
       "        17,  1],\n",
       "       [10,  1,  2,  4, 15,  5, 12, 16,  3, 17, 13,  9,  0,  6,  7,  8,\n",
       "        11, 14],\n",
       "       [11, 13,  6,  9, 14, 17,  3,  7, 10,  4,  8,  0, 16,  5,  1,  2,\n",
       "        15, 12],\n",
       "       [12, 17,  8, 14,  0, 16,  1,  7,  9,  5,  4, 10, 15, 11,  2,  6,\n",
       "         3, 13],\n",
       "       [13, 11,  3, 12, 16, 15,  9,  5,  8, 10,  0,  7,  2, 14, 17,  6,\n",
       "         4,  1],\n",
       "       [14,  3, 16,  8, 17,  1, 11,  2, 15,  5,  9, 12,  7,  0,  6, 13,\n",
       "        10,  4],\n",
       "       [15, 14,  1,  6, 10, 16,  3, 12,  0,  2, 17,  4,  9,  7,  5,  8,\n",
       "        11, 13],\n",
       "       [16, 14,  8,  2, 11, 12,  9,  3, 15,  6,  7, 17,  5,  0,  1, 13,\n",
       "         4, 10],\n",
       "       [17,  3,  1,  5,  0, 15, 16, 11, 10,  7,  8, 14, 13,  9,  4,  6,\n",
       "        12,  2]])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 10, 11, 12, 15, 16, 17]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(coefs.sum(1) > 0)[0].tolist() + [15, 16, 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
