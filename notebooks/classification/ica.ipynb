{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array, as_float_array\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "# https://lightgbm.readthedocs.io/en/latest/Python-API.html#lightgbm.LGBMModel\n",
    "from lightgbm import LGBMClassifier \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxentropyDichtomizationTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_splits, verbose=False):\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "        self.n_samples = None\n",
    "        self.n_features = None\n",
    "        self._splits = None\n",
    "        self._splits_indices = None\n",
    "    \n",
    "    def _check_X(self, X, n_features=None):\n",
    "        _X = None\n",
    "        if not hasattr(X, 'dtype'):\n",
    "            _X = check_array(as_float_array(X))\n",
    "        _X = check_array(X)\n",
    "        \n",
    "        if self.n_features:\n",
    "            if _X.shape[1] != self.n_features:\n",
    "                raise Exception('X has {} columns while {} are expected'.format(_X.shape[1], self.n_features))\n",
    "        return _X\n",
    "    \n",
    "    def _get_maxentropy_split(self, X):\n",
    "        block_size = X.shape[0]\n",
    "        ix_max_entropy = -1\n",
    "        max_entropy = -1\n",
    "        max_probas = None\n",
    "        for i in range(1, block_size - 1):\n",
    "            a = np.sum(X < X[i])\n",
    "            b = np.sum(X >= X[i])\n",
    "\n",
    "            p = np.array([a / block_size, b / block_size])\n",
    "            e = -np.sum(np.log(p + 1) * p)\n",
    "\n",
    "            if max_entropy < e:\n",
    "                max_probas = p\n",
    "                max_entropy = e\n",
    "                ix_max_entropy = i\n",
    "        \n",
    "        if max_probas is None or 0 in max_probas:\n",
    "            return 0, -1\n",
    "\n",
    "        return max_entropy, ix_max_entropy\n",
    "\n",
    "    def _dichtomize(self, X):\n",
    "        _iters = np.log2(self.n_splits)\n",
    "        if _iters - int(_iters) != 0:\n",
    "            raise Exception('number of bins should be of a power of 2')\n",
    "        \n",
    "        # make first maxentropy split\n",
    "        _, initial_bin = self._get_maxentropy_split(X)\n",
    "        splits_current_feature = [(0, initial_bin), (initial_bin, self.n_samples - 1)]\n",
    "        for i in range(int(_iters) - 1):\n",
    "            # an empty list for splits in current iteration\n",
    "            _splits = list()\n",
    "            for j in splits_current_feature:\n",
    "                entropy, index = self._get_maxentropy_split(X[j[0]: j[1]])\n",
    "                if entropy == 0:\n",
    "                    _splits += [(j[0], j[1])]\n",
    "                else:\n",
    "                    _splits += [(j[0], j[0] + index), (j[0] + index, j[1])]\n",
    "\n",
    "            splits_current_feature = _splits\n",
    "            \n",
    "        return splits_current_feature\n",
    "    \n",
    "    def _convert(self, X, ix):\n",
    "        result = list()\n",
    "        for x in X.flatten():\n",
    "            result.append(np.argwhere([k[0] <= x and x < k[1] for k in self._splits[ix]]))\n",
    "        return np.array(result).reshape(-1, 1) \n",
    "    \n",
    "    def fit(self, X):\n",
    "        X = self._check_X(X)\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        self._splits = list()\n",
    "        self._splits_indices = list()\n",
    "        \n",
    "        for ix in range(self.n_features):\n",
    "            x = np.sort(X[:, ix].flatten())\n",
    "            _indices = self._dichtomize(x.flatten())\n",
    "            \n",
    "            self._splits_indices.append(_indices)\n",
    "            self._splits.append([[x[i[0]], x[i[1]]] for i in _indices])\n",
    "            \n",
    "            self._splits[-1][0][0] = -np.inf\n",
    "            self._splits[-1][-1][1] = np.inf\n",
    "            \n",
    "            self._splits = np.array(self._splits)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        _, n_features = X.shape\n",
    "        X = self._check_X(X, n_features)\n",
    "        \n",
    "        X_categorical = list()\n",
    "        for ix in range(n_features):\n",
    "            X_categorical.append(self._convert(X, ix))\n",
    "            \n",
    "        return np.hstack(X_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF regressor optimization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt for lightgbm shows terrible results\n",
    "def hyperopt_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    space = hp.choice('clr_type', [\n",
    "        {\n",
    "            'type': 'lightgbm',\n",
    "            'feature_fraction': hp.uniform('feature_fraction', 0.05, 0.95),\n",
    "            'bagging_fraction': hp.uniform('bagging_fraction', 0.05, 0.95),\n",
    "            'bagging_freq': hp.uniform('bagging_freq', 1, 50),\n",
    "            'n_estimators': hp.uniform('n_estimators', 5, 50),\n",
    "            #'max_bin': hp.uniform('max_bin', )\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    best = fmin(\n",
    "        fn=lambda args: cross_val_score(\n",
    "            LGBMClassifier(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=args['feature_fraction'], \n",
    "                bagging_freq=int(args['bagging_freq']), \n",
    "                bagging_fraction=args['bagging_fraction'],\n",
    "                n_estimators=int(args['n_estimators'])\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='accuracy'\n",
    "        ).mean(),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_iter_opt\n",
    "    )\n",
    "    \n",
    "    return best\n",
    "\n",
    "def bayesian_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    svr_opt = BayesianOptimization(\n",
    "        lambda feature_fraction, bagging_freq, bagging_fraction, n_estimators: cross_val_score(\n",
    "            LGBMClassifier(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=feature_fraction, \n",
    "                bagging_freq=int(bagging_freq), \n",
    "                bagging_fraction=bagging_fraction,\n",
    "                n_estimators=int(n_estimators)\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='accuracy'\n",
    "        ).mean(),\n",
    "        {'feature_fraction': (0.05, 0.95),\n",
    "         'bagging_fraction': (0.05, 0.95),\n",
    "         'bagging_freq': (1, 50),\n",
    "         'n_estimators': (5, 50) },\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    svr_opt.init(10)\n",
    "    svr_opt.maximize(n_iter=max_iter_opt)\n",
    "    \n",
    "    return svr_opt.res['max']['max_params']#['C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Comment Volume Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/facebook_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (40949, 54)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical variables\n",
    "# H Local - category\n",
    "# Post Promotion Status - category\n",
    "# Base Time - time variable\n",
    "# Page Category - category\n",
    "\n",
    "H_Local = OneHotEncoder().fit_transform(df['H Local'].values.reshape(-1, 1)).todense()\n",
    "Post_Promotion_Status = OneHotEncoder().fit_transform(df['Post Promotion Status'].values.reshape(-1, 1)).todense()\n",
    "Base_Time = OneHotEncoder().fit_transform(df['Base Time'].values.reshape(-1, 1)).todense()\n",
    "Page_Category = OneHotEncoder().fit_transform(df['Page Category'].values.reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dichtomizer = MaxentropyDichtomizationTransformer(n_splits=32)\n",
    "y = dichtomizer.fit(df.Target.values.reshape(-1, 1)).transform(df.Target.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['H Local', 'Post Promotion Status', 'Base Time', 'Page Category', 'Target'], axis=1).values\n",
    "X = np.hstack([X, H_Local, Post_Promotion_Status, Base_Time, Page_Category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after preprocessing: (40949, 228)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size after preprocessing: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with ICA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 10\n",
    "ica = FastICA(n_components=10)\n",
    "X_ica = ica.fit_transform(X_train)\n",
    "X_val_ica = ica.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ica, y_train = shuffle(X_ica, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_ica, y_train, cv=6, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LogisticRegression(),\n",
    "    'svr': LinearSVC(C=100, dual=False, max_iter=200),\n",
    "    'forest': LGBMClassifier(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_ica, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_ica[train_ix], X_ica[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ICA data (averate scores):\n",
      "linear, Accuracy: 0.550197864338481\n",
      "svr, Accuracy: 0.5646610493141277\n",
      "forest, Accuracy: 0.5771152890770817\n"
     ]
    }
   ],
   "source": [
    "print('for ICA data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ICA data (validation scores):\n",
      "linear, Accuracy: 0.5623931623931624\n",
      "svr, Accuracy: 0.5758241758241758\n",
      "forest, Accuracy: 0.5885225885225885\n"
     ]
    }
   ],
   "source": [
    "print('for ICA data (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_ica, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val_ica), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parkinsons Telemonitoring Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/parkinsons_updrs.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (5875, 22)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = df['subject#'].unique().tolist()\n",
    "subject_binary = np.zeros((df.shape[0], len(subject)))\n",
    "for k, i in df.iterrows():\n",
    "    subject_binary[k, int(i['subject#']) - 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dichtomizer = MaxentropyDichtomizationTransformer(n_splits=32)\n",
    "y = dichtomizer.fit(df.total_UPDRS.values.reshape(-1, 1)).transform(df.total_UPDRS.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing categorical features with binary values\n",
    "X = df.drop(['motor_UPDRS', 'total_UPDRS', 'subject#'], axis=1).values\n",
    "X = np.concatenate([subject_binary, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after preprocessing: (5875, 61)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size after preprocessing: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with ICA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 10\n",
    "ica = FastICA(n_components=10)\n",
    "X_ica = ica.fit_transform(X_train)\n",
    "X_val_ica = ica.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ica, y_train = shuffle(X_ica, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_ica, y_train, cv=6, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LogisticRegression(),\n",
    "    'svr': LinearSVC(C=100, dual=False, max_iter=200),\n",
    "    'forest': LGBMClassifier(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_ica, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_ica[train_ix], X_ica[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ICA data (averate scores):\n",
      "linear, Accuracy: 0.13183050245266584\n",
      "svr, Accuracy: 0.22470143950854993\n",
      "forest, Accuracy: 0.6487616329711641\n"
     ]
    }
   ],
   "source": [
    "print('for ICA data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using ICA (validation scores):\n",
      "linear, Accuracy: 0.13945578231292516\n",
      "svr, Accuracy: 0.23979591836734693\n",
      "forest, Accuracy: 0.7278911564625851\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using ICA (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_ica, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val_ica), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy efficiency Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../datasets/ENB2012_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (768, 10)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = df['X6'].unique().tolist()\n",
    "subject_map = dict(zip(subject, range(len(subject))))\n",
    "subject_binary = np.zeros((df.shape[0], len(subject)))\n",
    "for k, i in df.iterrows():\n",
    "    subject_binary[k, subject_map[i['X6']]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dichtomizer = MaxentropyDichtomizationTransformer(n_splits=32)\n",
    "y = dichtomizer.fit(df.Y1.values.reshape(-1, 1)).transform(df.Y1.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing categorical features with binary values\n",
    "X = df.drop(['Y1', 'Y2', 'X6'], axis=1).values\n",
    "X = np.concatenate([subject_binary, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after preprocessing: (768, 11)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size after preprocessing: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with ICA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 4\n",
    "ica = FastICA(n_components=4)\n",
    "X_ica = ica.fit_transform(X_train)\n",
    "X_val_ica = ica.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ica, y_train = shuffle(X_ica, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_ica, y_train, cv=6, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LogisticRegression(),\n",
    "    'svr': LinearSVC(C=100, dual=False, max_iter=200),\n",
    "    'forest': LGBMClassifier(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_ica, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_ica[train_ix], X_ica[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ICA data (averate scores):\n",
      "linear, Accuracy: 0.03331328521785619\n",
      "svr, Accuracy: 0.11003074044373162\n",
      "forest, Accuracy: 0.1476209569633788\n"
     ]
    }
   ],
   "source": [
    "print('for ICA data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using ICA (validation scores):\n",
      "linear, Accuracy: 0.025974025974025976\n",
      "svr, Accuracy: 0.12987012987012986\n",
      "forest, Accuracy: 0.19480519480519481\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using ICA (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_ica, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val_ica), y_val.squeeze())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
