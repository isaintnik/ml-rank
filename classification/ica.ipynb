{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "# https://lightgbm.readthedocs.io/en/latest/Python-API.html#lightgbm.LGBMModel\n",
    "from lightgbm import LGBMClassifier \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedianBinarizer(object):\n",
    "    def __init__(self, n_bins: int):\n",
    "        self.n_bins = n_bins\n",
    "        self.x_bins = None\n",
    "        self.encoder = None\n",
    "    \n",
    "    def _get_feature_bins(self, X):\n",
    "        #extra_bin = 0\n",
    "        #if X.shape[0] % self.n_bins != 0:\n",
    "        #    extra_bin = 1\n",
    "        #    print('number of bins does not fit to dataset'\n",
    "        #          ', extending number of bins to extra one')\n",
    "        #\n",
    "        #n_batch = X.shape[0] // self.n_bins\n",
    "        #X_sorted = np.sort(X)\n",
    "        #x_bins = [X[i*n_batch:(i+1)*n_batch] for i in range(self.n_bins + extra_bin)]\n",
    "        \n",
    "        iterations = np.log2(self.n_bins)\n",
    "        if iterations - int(iterations) != 0:\n",
    "            raise Exception('bins should be a power of 2')\n",
    "        \n",
    "        # получаем вариационный ряд\n",
    "        x_bins = [np.sort(X)]\n",
    "        for i in range(int(iterations)):\n",
    "            new_bins = list()\n",
    "            for j in x_bins:\n",
    "                # середина среза вариационного ряда\n",
    "                index = j.shape[0] // 2\n",
    "                new_bins += [j[:index], j[index:]]\n",
    "            x_bins = new_bins\n",
    "        return x_bins\n",
    "    \n",
    "    def _lookup(self, x):\n",
    "        for k, _bin in enumerate(self.x_bins):\n",
    "            if x <= _bin[-1]:\n",
    "                return k\n",
    "        return k\n",
    "    \n",
    "    def _real_to_category(self, X):\n",
    "        # X should not be a column vector\n",
    "        X_new = []\n",
    "        for x in X:\n",
    "            i = self._lookup(x)\n",
    "            \n",
    "            if i != -1:\n",
    "                X_new.append(i)\n",
    "            else:\n",
    "                raise Exception('something went wrong')\n",
    "\n",
    "        return np.array(X_new).reshape(-1, 1)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.x_bins = self._get_feature_bins(X)\n",
    "        X_cat = self._real_to_category(X)\n",
    "\n",
    "        self.encoder = OneHotEncoder(n_values=self.n_bins, sparse=False)\n",
    "        self.encoder.fit(X_cat)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform_categorical(self, X):\n",
    "        return self._real_to_category(X)\n",
    "    \n",
    "    def transform_onehot(self, X):\n",
    "        x_cat = self._real_to_category(X)\n",
    "        return self.encoder.transform(x_cat)\n",
    "    \n",
    "    def onehot_to_categorical(self, X):\n",
    "        return np.argwhere(X)[:, 1]\n",
    "    \n",
    "    def categorical_to_onehot(self, X):\n",
    "        return self.encoder.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF regressor optimization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt for lightgbm shows terrible results\n",
    "def hyperopt_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    space = hp.choice('clr_type', [\n",
    "        {\n",
    "            'type': 'lightgbm',\n",
    "            'feature_fraction': hp.uniform('feature_fraction', 0.05, 0.95),\n",
    "            'bagging_fraction': hp.uniform('bagging_fraction', 0.05, 0.95),\n",
    "            'bagging_freq': hp.uniform('bagging_freq', 1, 50),\n",
    "            'n_estimators': hp.uniform('n_estimators', 5, 50),\n",
    "            #'max_bin': hp.uniform('max_bin', )\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    best = fmin(\n",
    "        fn=lambda args: cross_val_score(\n",
    "            LGBMClassifier(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=args['feature_fraction'], \n",
    "                bagging_freq=int(args['bagging_freq']), \n",
    "                bagging_fraction=args['bagging_fraction'],\n",
    "                n_estimators=int(args['n_estimators'])\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='accuracy'\n",
    "        ).mean(),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_iter_opt\n",
    "    )\n",
    "    \n",
    "    return best\n",
    "\n",
    "def bayesian_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    svr_opt = BayesianOptimization(\n",
    "        lambda feature_fraction, bagging_freq, bagging_fraction, n_estimators: cross_val_score(\n",
    "            LGBMClassifier(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=feature_fraction, \n",
    "                bagging_freq=int(bagging_freq), \n",
    "                bagging_fraction=bagging_fraction,\n",
    "                n_estimators=int(n_estimators)\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='accuracy'\n",
    "        ).mean(),\n",
    "        {'feature_fraction': (0.05, 0.95),\n",
    "         'bagging_fraction': (0.05, 0.95),\n",
    "         'bagging_freq': (1, 50),\n",
    "         'n_estimators': (5, 50) },\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    svr_opt.init(10)\n",
    "    svr_opt.maximize(n_iter=max_iter_opt)\n",
    "    \n",
    "    return svr_opt.res['max']['max_params']#['C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Comment Volume Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/facebook_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (40949, 54)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = MedianBinarizer(n_bins=32).fit(df.Target.values.reshape(-1, 1)).transform_categorical(df.Target.values.reshape(-1, 1))\n",
    "df.drop(['Target'], axis=1, inplace=True)\n",
    "\n",
    "categorical_features = ['H Local', 'Post Promotion Status', 'Base Time', 'Page Category']\n",
    "\n",
    "\n",
    "features_categorical = [\n",
    "    OneHotEncoder().fit_transform(df[i].values.reshape(-1, 1)).todense() for i in categorical_features\n",
    "]\n",
    "\n",
    "features_real = [\n",
    "    MedianBinarizer(n_bins=32).fit(df[i].values.reshape(-1, 1)).transform_onehot(df[i].values.reshape(-1, 1))\n",
    "    for i in np.setdiff1d(ar1=df.columns, ar2=categorical_features)\n",
    "]\n",
    "\n",
    "X = np.concatenate(features_categorical + features_real, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after preprocessing: (40949, 1747)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size after preprocessing: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with ICA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 10\n",
    "ica = FastICA(n_components=10)\n",
    "X_ica = ica.fit_transform(X_train)\n",
    "X_val_ica = ica.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ica, y_train = shuffle(X_ica, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_ica, y_train, cv=6, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LogisticRegression(),\n",
    "    'svr': LinearSVC(C=100, dual=False, max_iter=200),\n",
    "    'forest': LGBMClassifier(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_ica, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_ica[train_ix], X_ica[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for pure data (averate scores):\n",
      "linear, Accuracy: 0.5512014179935929\n",
      "svr, Accuracy: 0.6526295942731336\n",
      "forest, Accuracy: 0.6601722694259458\n"
     ]
    }
   ],
   "source": [
    "print('for ICA data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ICA data (validation scores):\n",
      "linear, Accuracy: 0.5531135531135531\n",
      "svr, Accuracy: 0.6490842490842491\n",
      "forest, Accuracy: 0.6615384615384615\n"
     ]
    }
   ],
   "source": [
    "print('for ICA data (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_ica, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val_ica), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parkinsons Telemonitoring Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/parkinsons_updrs.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (5875, 22)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = MedianBinarizer(n_bins=32).fit(df.total_UPDRS.values.reshape(-1, 1)).transform_categorical(df.total_UPDRS.values.reshape(-1, 1))\n",
    "df.drop(['total_UPDRS'], axis=1, inplace=True)\n",
    "\n",
    "categorical_features = ['subject#']\n",
    "\n",
    "\n",
    "features_categorical = [\n",
    "    OneHotEncoder().fit_transform(df[i].values.reshape(-1, 1)).todense() for i in categorical_features\n",
    "]\n",
    "\n",
    "features_real = [\n",
    "    MedianBinarizer(n_bins=32).fit(df[i].values.reshape(-1, 1)).transform_onehot(df[i].values.reshape(-1, 1))\n",
    "    for i in np.setdiff1d(ar1=df.columns, ar2=categorical_features)\n",
    "]\n",
    "\n",
    "X = np.concatenate(features_categorical + features_real, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after preprocessing: (5875, 682)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size after preprocessing: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with ICA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 10\n",
    "ica = FastICA(n_components=10)\n",
    "X_ica = ica.fit_transform(X_train)\n",
    "X_val_ica = ica.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ica, y_train = shuffle(X_ica, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_ica, y_train, cv=6, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LogisticRegression(),\n",
    "    'svr': LinearSVC(C=100, dual=False, max_iter=200),\n",
    "    'forest': LGBMClassifier(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_ica, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_ica[train_ix], X_ica[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ICA data (averate scores):\n",
      "linear, Accuracy: 0.4787188259294916\n",
      "svr, Accuracy: 0.5057692087287398\n",
      "forest, Accuracy: 0.6731536239856966\n"
     ]
    }
   ],
   "source": [
    "print('for ICA data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using ICA (validation scores):\n",
      "linear, Accuracy: 0.4642857142857143\n",
      "svr, Accuracy: 0.4880952380952381\n",
      "forest, Accuracy: 0.6564625850340136\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using ICA (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_ica, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val_ica), y_val.squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy efficiency Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../datasets/ENB2012_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (768, 10)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = MedianBinarizer(n_bins=32).fit(df.Y1.values.reshape(-1, 1)).transform_categorical(df.Y1.values.reshape(-1, 1))\n",
    "df.drop(['Y1', 'Y2'], axis=1, inplace=True)\n",
    "\n",
    "categorical_features = ['X6']\n",
    "\n",
    "\n",
    "features_categorical = [\n",
    "    OneHotEncoder().fit_transform(df[i].values.reshape(-1, 1)).todense() for i in categorical_features\n",
    "]\n",
    "\n",
    "features_real = [\n",
    "    MedianBinarizer(n_bins=32).fit(df[i].values.reshape(-1, 1)).transform_onehot(df[i].values.reshape(-1, 1))\n",
    "    for i in np.setdiff1d(ar1=df.columns, ar2=categorical_features)\n",
    "]\n",
    "\n",
    "X = np.concatenate(features_categorical + features_real, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after preprocessing: (768, 228)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size after preprocessing: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with ICA-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components is fixed to 4\n",
    "ica = FastICA(n_components=4)\n",
    "X_ica = ica.fit_transform(X_train)\n",
    "X_val_ica = ica.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ica, y_train = shuffle(X_ica, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Estimating LightGBM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = bayesian_optimization_lightgbm(X_ica, y_train, cv=6, max_iter_opt=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt['bagging_freq'] = int(params_opt['bagging_freq'])\n",
    "params_opt['n_estimators'] = int(params_opt['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Evaluate with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'linear': LogisticRegression(),\n",
    "    'svr': LinearSVC(C=100, dual=False, max_iter=200),\n",
    "    'forest': LGBMClassifier(boosting_type='rf', **params_opt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for k, model in models.items():\n",
    "    stats[k] = []\n",
    "    kfold = KFold(n_splits=8, shuffle=True)\n",
    "    \n",
    "    for train_ix, test_ix in kfold.split(X_ica, y_train):\n",
    "        X_crossval_train, X_crossval_test = X_ica[train_ix], X_ica[test_ix]\n",
    "        y_crossval_train, y_crossval_test = y_train[train_ix], y_train[test_ix]\n",
    "        \n",
    "        # here must be sume sort of optimization\n",
    "        model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "        stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ICA data (averate scores):\n",
      "linear, Accuracy: 0.5398623362737236\n",
      "svr, Accuracy: 0.7220662924351777\n",
      "forest, Accuracy: 0.7611601176156109\n"
     ]
    }
   ],
   "source": [
    "print('for ICA data (averate scores):')\n",
    "for model, model_stats in stats.items():\n",
    "    print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for decorrelated data using ICA (validation scores):\n",
      "linear, Accuracy: 0.5714285714285714\n",
      "svr, Accuracy: 0.7662337662337663\n",
      "forest, Accuracy: 0.7922077922077922\n"
     ]
    }
   ],
   "source": [
    "print('for decorrelated data using ICA (validation scores):')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_ica, y_train.squeeze())\n",
    "    print('{}, Accuracy: {}'.format(name, accuracy_score(model.predict(X_val_ica), y_val.squeeze())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
