{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array, as_float_array\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from copy import deepcopy as copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс дискретизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxentropyMedianDichtomizationTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_splits, verbose=False):\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "        self.n_samples = None\n",
    "        self.n_features = None\n",
    "        self._splits = None\n",
    "        self._splits_indices = None\n",
    "    \n",
    "    def _check_X(self, X, n_features=None):\n",
    "        _X = None\n",
    "        if not hasattr(X, 'dtype'):\n",
    "            _X = check_array(as_float_array(X))\n",
    "        _X = check_array(X)\n",
    "        \n",
    "        if self.n_features:\n",
    "            if _X.shape[1] != self.n_features:\n",
    "                raise Exception('X has {} columns while {} are expected'.format(_X.shape[1], self.n_features))\n",
    "        return _X\n",
    "    \n",
    "    def _calc_entropy(self, X, split_bias):\n",
    "        a = np.sum(X < split_bias)\n",
    "        b = np.sum(X >= split_bias)\n",
    "\n",
    "        p = np.array([a / X.shape[0], b / X.shape[0]])\n",
    "        return -np.sum(np.log(p + 1) * p)\n",
    "\n",
    "\n",
    "    def _get_maxentropy_split(self, X):\n",
    "        # O(n^2)\n",
    "        block_size = X.shape[0]\n",
    "        ix_max_entropy = -1\n",
    "        X_diff = np.diff(X)\n",
    "\n",
    "        start_point = X.shape[0] // 2\n",
    "        \n",
    "        right_point = right_entropy = None\n",
    "        left_point = left_entropy =None\n",
    "        \n",
    "        max_entropy = -1\n",
    "        max_probas = None\n",
    "        # define point where to start looking for\n",
    "        # highest entropy\n",
    "        if X_diff[start_point] == 0:\n",
    "            _right_indices = np.where(X_diff[start_point:] > 0)[0]\n",
    "            _left_indices = np.where(X_diff[:start_point] > 0)[0]\n",
    "            \n",
    "            if _right_indices.any():\n",
    "                right_point = _right_indices[0] + start_point\n",
    "            \n",
    "            if _left_indices.any():\n",
    "                left_point = _left_indices[-1]\n",
    "            \n",
    "            # if we have constant series\n",
    "            if _right_indices is None and _left_indices is None:\n",
    "                return 0, -1\n",
    "            \n",
    "            if right_point:\n",
    "                right_entropy = self._calc_entropy(X, X[right_point])\n",
    "        \n",
    "            if left_point:\n",
    "                left_entropy = self._calc_entropy(X, X[left_point])\n",
    "        else:\n",
    "            right_point = start_point + 1\n",
    "            left_point = start_point - 1\n",
    "            \n",
    "            right_entropy = self._calc_entropy(X, X[right_point])\n",
    "            left_entropy = self._calc_entropy(X, X[left_point])\n",
    "            center_entropy = self._calc_entropy(X, X[start_point])\n",
    "            \n",
    "            if center_entropy > left_entropy and center_entropy > right_entropy:\n",
    "                return center_entropy, start_point\n",
    "\n",
    "        # if entropy at the point left to the starting point is higher\n",
    "        # search for entropy maxima\n",
    "        if right_point and  (not left_point or right_entropy > left_entropy):\n",
    "            for j in range(right_point + 1, block_size):\n",
    "                local_entropy = self._calc_entropy(X, X[j])\n",
    "                if local_entropy > right_entropy:\n",
    "                    right_point = j\n",
    "                    right_entropy = local_entropy\n",
    "                else:\n",
    "                    return right_entropy, right_point\n",
    "        elif left_point:\n",
    "            for j in reversed(range(0, left_point - 1)):\n",
    "                local_entropy = self._calc_entropy(X, X[j])\n",
    "                if local_entropy > left_entropy:\n",
    "                    left_point = j\n",
    "                    left_entropy = local_entropy\n",
    "                else:\n",
    "                    return left_entropy, left_point\n",
    "        \n",
    "        return 0, -1\n",
    "\n",
    "    def _dichtomize(self, X):\n",
    "        # O(n^2 * log n)\n",
    "        \n",
    "        _iters = np.log2(self.n_splits)\n",
    "        if _iters - int(_iters) != 0:\n",
    "            raise Exception('number of bins should be of a power of 2')\n",
    "        \n",
    "        # make first maxentropy split\n",
    "        _, initial_bin = self._get_maxentropy_split(X)\n",
    "        splits_current_feature = [(0, initial_bin), (initial_bin, self.n_samples - 1)]\n",
    "        for i in range(int(_iters) - 1):\n",
    "            # an empty list for splits in current iteration\n",
    "            _splits = list()\n",
    "            for j in splits_current_feature:\n",
    "                entropy, index = self._get_maxentropy_split(X[j[0]: j[1]])\n",
    "                if entropy == 0:\n",
    "                    _splits += [(j[0], j[1])]\n",
    "                else:\n",
    "                    _splits += [(j[0], j[0] + index), (j[0] + index, j[1])]\n",
    "\n",
    "            splits_current_feature = _splits\n",
    "            \n",
    "        return splits_current_feature\n",
    "    \n",
    "    def _convert(self, X, ix):\n",
    "        result = list()\n",
    "        for x in X.flatten():\n",
    "            result.append(np.argwhere([k[0] <= x and x < k[1] for k in self._splits[ix]]))\n",
    "        return np.array(result).reshape(-1, 1) \n",
    "    \n",
    "    def fit(self, X):\n",
    "        X = self._check_X(X)\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        self._splits = list()\n",
    "        self._splits_indices = list()\n",
    "        \n",
    "        for ix in range(self.n_features):\n",
    "            x = np.sort(X[:, ix].flatten())\n",
    "            _indices = self._dichtomize(x.flatten())\n",
    "            \n",
    "            self._splits_indices.append(_indices)\n",
    "            self._splits.append([[x[i[0]], x[i[1]]] for i in _indices])\n",
    "            \n",
    "            self._splits[-1][0][0] = -np.inf\n",
    "            self._splits[-1][-1][1] = np.inf\n",
    "            \n",
    "            self._splits = np.array(self._splits)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        _, n_features = X.shape\n",
    "        X = self._check_X(X, n_features)\n",
    "        \n",
    "        X_categorical = list()\n",
    "        for ix in range(n_features):\n",
    "            X_categorical.append(self._convert(X, ix))\n",
    "            \n",
    "        return np.hstack(X_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeFabulousTransformation(BaseEstimator):\n",
    "    def __init__(self, base_estimator, dichtomized=False, n_splits=32, exhausitve=True, random_seed=42, verbose=False):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_splits = n_splits\n",
    "        self.random_seed = random_seed\n",
    "        self.exhausitve = exhausitve\n",
    "        self.verbose = verbose\n",
    "        self.dichtomized=dichtomized\n",
    "        self._feature_space = None\n",
    "        self._feature_dichtomizers = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _cross_entropy(p, q):\n",
    "        p = np.array(p)\n",
    "        q = np.array(q)\n",
    "        q[q == 0] = 1e-8\n",
    "        return -np.sum(p * np.log(q))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _synchronize_two_dicts(_from:dict, _to:dict):\n",
    "        for i in np.setdiff1d(list(_from.keys()), list(_to.keys())):\n",
    "            _to[i] = 0\n",
    "            \n",
    "    def _dichtomize(self, X):\n",
    "        self._feature_dichtomizers = list()\n",
    "        self._feature_space = list()\n",
    "        \n",
    "        if not self.dichtomized:\n",
    "            for i in range(X.shape[1]):\n",
    "                feature = X[:, i].reshape(-1, 1)\n",
    "                dichtomizer = MaxentropyMedianDichtomizationTransformer(32).fit(feature)\n",
    "                feature_dichtomized = dichtomizer.transform(feature)\n",
    "                onehot_encoder = OneHotEncoder(sparse=True).fit(feature_dichtomized)\n",
    "\n",
    "                self._feature_dichtomizers.append({'dichtomizer': dichtomizer, 'encoder': onehot_encoder})\n",
    "                self._feature_space.append({'categorical': feature_dichtomized, 'binary': onehot_encoder.transform(feature_dichtomized)})\n",
    "        else:\n",
    "            for i in range(X.shape[1]):\n",
    "                feature = X[:, i].reshape(-1, 1)\n",
    "                onehot_encoder = OneHotEncoder(sparse=True).fit(feature)\n",
    "                self._feature_dichtomizers.append({'dichtomizer': None, 'encoder': onehot_encoder})\n",
    "                self._feature_space.append({'categorical': feature, 'binary': onehot_encoder.transform(feature)})\n",
    "                \n",
    "    \n",
    "    def _calc_cross_entropy(self, ix_feature, predicted, dataset_size):\n",
    "        pred_category, pred_counts = np.unique(predicted, return_counts=True)\n",
    "        real_category, real_counts = np.unique(self._feature_space[ix_feature]['categorical'], return_counts=True)\n",
    "\n",
    "        pred_proba = pred_counts / dataset_size\n",
    "        real_proba = real_counts / dataset_size\n",
    "\n",
    "        real_stats = dict(zip(real_category, real_proba))\n",
    "        pred_stats =  dict(zip(pred_category, pred_proba))\n",
    "\n",
    "        SomeFabulousTransformation._synchronize_two_dicts(real_stats, pred_stats)\n",
    "        \n",
    "        return SomeFabulousTransformation._cross_entropy(list(real_stats.values()), list(pred_stats.values()))\n",
    "    \n",
    "    def _fit_transform(self, X, initial_feature_ix):\n",
    "        dataset_size = X.shape[0]\n",
    "        free_features_ix = [i for i in range(len(self._feature_space)) if i != initial_feature_ix]\n",
    "        active_features_subset = [self._feature_space[initial_feature_ix]['binary']]\n",
    "        active_features_subset_ix = [initial_feature_ix]\n",
    "        \n",
    "        while len(active_features_subset) != len(self._feature_space):\n",
    "            max_entropy = -1\n",
    "            max_entropy_feature_ix = -1\n",
    "            max_entropy_feature_value = None\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('currently processed {} features out of {}'.format(len(active_features_subset), len(self._feature_space)))\n",
    "            \n",
    "            for ix_current_feature in free_features_ix:\n",
    "                if len(active_features_subset) > 1:\n",
    "                    model_input_features = np.concatenate(active_features_subset, axis=1)\n",
    "                else:\n",
    "                    model_input_features = active_features_subset[0]\n",
    "                \n",
    "                estimator = copy(self.base_estimator)\n",
    "                estimator.fit(model_input_features, self._feature_space[ix_current_feature]['categorical'].squeeze())\n",
    "                \n",
    "                pred = estimator.predict(model_input_features)\n",
    "\n",
    "                pred_onehot = self._feature_dichtomizers[ix_current_feature]['encoder'].transform(pred.reshape(-1, 1))\n",
    "                pred_diff = (pred_onehot != self._feature_space[ix_current_feature]['binary']).astype(np.int32)\n",
    "                entropy = self._calc_cross_entropy(ix_current_feature, pred_diff, dataset_size)\n",
    "                \n",
    "                if entropy > max_entropy:\n",
    "                    max_entropy_feature_value = pred_diff\n",
    "                    max_entropy_feature_ix = ix_current_feature\n",
    "                    max_entropy = entropy\n",
    "            \n",
    "            free_features_ix.remove(max_entropy_feature_ix)\n",
    "            active_features_subset.append(max_entropy_feature_value)\n",
    "            active_features_subset_ix.append(max_entropy_feature_ix)\n",
    "        \n",
    "        return np.hstack(active_features_subset), active_features_subset_ix      \n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        self._dichtomize(X)\n",
    "        \n",
    "        if not self.exhausitve:\n",
    "            initial_feature_ix = np.random.randint(0, len(self._feature_space))\n",
    "            return self._fit_transform(X, initial_feature_ix)\n",
    "        \n",
    "        features_subset = list()\n",
    "        features_subset_ix = list()\n",
    "        for initial_feature_ix in range(len(feature_space)):\n",
    "            features_subset, features_subset_ix = self._fit_transform(X, initial_feature_ix)\n",
    "            \n",
    "        return features_subset, np.vstack(features_subset_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF regressor optimization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt for lightgbm shows terrible results\n",
    "def hyperopt_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    space = hp.choice('clr_type', [\n",
    "        {\n",
    "            'type': 'lightgbm',\n",
    "            'feature_fraction': hp.uniform('feature_fraction', 0.05, 0.95),\n",
    "            'bagging_fraction': hp.uniform('bagging_fraction', 0.05, 0.95),\n",
    "            'bagging_freq': hp.uniform('bagging_freq', 1, 50),\n",
    "            'n_estimators': hp.uniform('n_estimators', 5, 50),\n",
    "            #'max_bin': hp.uniform('max_bin', )\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    best = fmin(\n",
    "        fn=lambda args: cross_val_score(\n",
    "            LGBMClassifier(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=args['feature_fraction'], \n",
    "                bagging_freq=int(args['bagging_freq']), \n",
    "                bagging_fraction=args['bagging_fraction'],\n",
    "                n_estimators=int(args['n_estimators'])\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='accuracy'\n",
    "        ).mean(),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_iter_opt\n",
    "    )\n",
    "    \n",
    "    return best\n",
    "\n",
    "def bayesian_optimization_lightgbm(X, y, cv=6, max_iter_opt=15):\n",
    "    svr_opt = BayesianOptimization(\n",
    "        lambda feature_fraction, bagging_freq, bagging_fraction, n_estimators: cross_val_score(\n",
    "            LGBMClassifier(\n",
    "                boosting_type='rf', \n",
    "                feature_fraction=feature_fraction, \n",
    "                bagging_freq=int(bagging_freq), \n",
    "                bagging_fraction=bagging_fraction,\n",
    "                n_estimators=int(n_estimators)\n",
    "            ),\n",
    "            X, y.squeeze(), cv=KFold(n_splits=cv).split(X), scoring='accuracy'\n",
    "        ).mean(),\n",
    "        {'feature_fraction': (0.05, 0.95),\n",
    "         'bagging_fraction': (0.05, 0.95),\n",
    "         'bagging_freq': (1, 50),\n",
    "         'n_estimators': (5, 50) },\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    svr_opt.init(10)\n",
    "    svr_opt.maximize(n_iter=max_iter_opt)\n",
    "    \n",
    "    return svr_opt.res['max']['max_params']#['C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = read_idx('./datasets/mnist/train-images.idx3-ubyte')\n",
    "y = read_idx('./datasets/mnist/train-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'rf': LGBMClassifier(boosting_type='rf', bagging_freq=5, bagging_fraction=.05, feature_fraction=.1),\n",
    "    'lr': LogisticRegression(random_state=42, multi_class='ovr', solver='sag', C=10000, tol=1e-2, n_jobs=10),\n",
    "    'svc': LinearSVC(multi_class='ovr'),\n",
    "}\n",
    "\n",
    "def score_models(models, X, y, folds = 8):\n",
    "    stats = {}\n",
    "\n",
    "    for k, model in models.items():\n",
    "        stats[k] = []\n",
    "        kfold = KFold(n_splits=8, shuffle=True)\n",
    "\n",
    "        for train_ix, test_ix in kfold.split(X, y):\n",
    "            X_crossval_train, X_crossval_test = X[train_ix], X[test_ix]\n",
    "            y_crossval_train, y_crossval_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            # here must be sume sort of optimization\n",
    "            model.fit(X_crossval_train, y_crossval_train.ravel())\n",
    "            stats[k].append(accuracy_score(model.predict(X_crossval_test), y_crossval_test, normalize=True))\n",
    "    \n",
    "    for model, model_stats in stats.items():\n",
    "        print('{}, Accuracy: {}'.format(model, np.mean(model_stats)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-35-2c250d31a506>\", line 2, in <module>\n",
      "    features, features_ix = transformer.fit_transform(X)\n",
      "  File \"<ipython-input-27-cdd4637370c1>\", line 102, in fit_transform\n",
      "    self._dichtomize(X)\n",
      "  File \"<ipython-input-27-cdd4637370c1>\", line 40, in _dichtomize\n",
      "    onehot_encoder = OneHotEncoder(sparse=False).fit(feature)\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 413, in fit\n",
      "    copy=True)\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\base.py\", line 45, in _transform_selected\n",
      "    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 522, in check_array\n",
      "    array = np.asarray(array, dtype=dtype, order=order)\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\numpy\\core\\numeric.py\", line 501, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\inspect.py\", line 1445, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 177, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"D:\\Program Files (x86)\\Anaconda\\lib\\tokenize.py\", line 452, in open\n",
      "    buffer = _builtin_open(filename, 'rb')\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "transformer = SomeFabulousTransformation(models['lr'], exhausitve=False, verbose=True, dichtomized=True)\n",
    "features, features_ix = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/creditcard/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[np.setdiff1d(df.columns, [\"Class\", \"Time\"])].values\n",
    "y = df.Class.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'rf': LGBMClassifier(boosting_type='rf', bagging_freq=5, bagging_fraction=.05, feature_fraction=.1),\n",
    "    'lr': LogisticRegression(random_state=42, multi_class='ovr', solver='sag', C=10000, tol=1e-2, n_jobs=10),\n",
    "    'svc': LinearSVC(multi_class='ovr', tol=1e-2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dichtomizing 0 feature out of 29\n",
      "dichtomizing 1 feature out of 29\n",
      "dichtomizing 2 feature out of 29\n",
      "dichtomizing 3 feature out of 29\n",
      "dichtomizing 4 feature out of 29\n",
      "dichtomizing 5 feature out of 29\n",
      "dichtomizing 6 feature out of 29\n",
      "dichtomizing 7 feature out of 29\n",
      "dichtomizing 8 feature out of 29\n",
      "dichtomizing 9 feature out of 29\n",
      "dichtomizing 10 feature out of 29\n",
      "dichtomizing 11 feature out of 29\n",
      "dichtomizing 12 feature out of 29\n",
      "dichtomizing 13 feature out of 29\n",
      "dichtomizing 14 feature out of 29\n",
      "dichtomizing 15 feature out of 29\n",
      "dichtomizing 16 feature out of 29\n",
      "dichtomizing 17 feature out of 29\n",
      "dichtomizing 18 feature out of 29\n",
      "dichtomizing 19 feature out of 29\n",
      "dichtomizing 20 feature out of 29\n",
      "dichtomizing 21 feature out of 29\n",
      "dichtomizing 22 feature out of 29\n",
      "dichtomizing 23 feature out of 29\n",
      "dichtomizing 24 feature out of 29\n",
      "dichtomizing 25 feature out of 29\n",
      "dichtomizing 26 feature out of 29\n",
      "dichtomizing 27 feature out of 29\n",
      "dichtomizing 28 feature out of 29\n",
      "currently processed 1 features out of 29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-84d35271f15b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSomeFabulousTransformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'svc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexhausitve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_ix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-dbacc8585c97>\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhausitve\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0minitial_feature_ix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feature_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_feature_ix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mfeatures_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-dbacc8585c97>\u001b[0m in \u001b[0;36m_fit_transform\u001b[1;34m(self, X, initial_feature_ix)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_input_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feature_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix_current_feature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'categorical'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_input_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"crammer_singer\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    915\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer = SomeFabulousTransformation(models['svc'], exhausitve=False, verbose=True, n_splits=8)\n",
    "features, features_ix = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
